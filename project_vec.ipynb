{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from verstack.stratified_continuous_split import scsplit # pip install verstack\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from scipy import spatial\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import os\n",
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基本思路\n",
    "使用如下两组要素\n",
    "- 数字变量\n",
    "  1. favorites count (log_stand)\n",
    "  2. followers count (log_stand)\n",
    "  3. statues count (发推数量) (log_stand)\n",
    "  4. friends count (log_stand)\n",
    "  5. urls count (log_stand)\n",
    "  6. verified (1 or 0) (这一部分我们不做log_stand)\n",
    "  7. timestamp (一天内的早中晚（hour），距离大选的时间(month和day)，周几(wday)都可能产生影响)\n",
    "- 文本变量\n",
    "  1. tweet text\n",
    "  2. hashtags \n",
    "\n",
    "将文本变量通过embedding处理后用CNN/LSTM得到一部分输出，另外对数字变量通过CNN也得到了一部分结果，最后汇总，用2层dense layer。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all mentions in train data:  {']', '['}\n",
      "average number of urls in train data: 0.529\n",
      "number of distinct urls in eval data:  185951\n",
      "verified users:  10621\n",
      "average number of hashtags in train data: 0.297\n",
      "number of distinct hashtags in train data: 12093\n",
      "average number of hashtags in evaluation data: 0.299\n",
      "number of distinct hashtags in evaluation data: 5942\n",
      "number of tweets not posted in 2022 in train data: 2380\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>retweets_count</th>\n",
       "      <th>favorites_count</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>statuses_count</th>\n",
       "      <th>friends_count</th>\n",
       "      <th>verified</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>urls_count</th>\n",
       "      <th>hashtags_count</th>\n",
       "      <th>month</th>\n",
       "      <th>wday</th>\n",
       "      <th>hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[rt, refarcir, macron, ans, nom, prépare]</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3682</td>\n",
       "      <td>453535</td>\n",
       "      <td>3628</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>1646978048000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[populaire]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>86</td>\n",
       "      <td>1016</td>\n",
       "      <td>284</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>1647694288000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[faut, dégager, cinglé]</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1944</td>\n",
       "      <td>28234</td>\n",
       "      <td>1995</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>1647370048000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[enseignants, mettre, prescriptions, président...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1072</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>1647256282000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[mafieuse, oppressive, macron]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13957</td>\n",
       "      <td>25311</td>\n",
       "      <td>10841</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>1647258374000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  retweets_count  \\\n",
       "0          [rt, refarcir, macron, ans, nom, prépare]               3   \n",
       "1                                        [populaire]               0   \n",
       "2                            [faut, dégager, cinglé]               3   \n",
       "3  [enseignants, mettre, prescriptions, président...               0   \n",
       "4                     [mafieuse, oppressive, macron]               0   \n",
       "\n",
       "   favorites_count  followers_count  statuses_count  friends_count  verified  \\\n",
       "0                0             3682          453535           3628         0   \n",
       "1                0               86            1016            284         0   \n",
       "2                1             1944           28234           1995         0   \n",
       "3                0                1            1072              0         0   \n",
       "4                0            13957           25311          10841         0   \n",
       "\n",
       "  hashtags      timestamp  urls_count  hashtags_count  month  wday  hour  \n",
       "0       []  1646978048000           0               0      3     4     5  \n",
       "1       []  1647694288000           0               0      3     5    12  \n",
       "2       []  1647370048000           0               0      3     1    18  \n",
       "3       []  1647256282000           1               0      3     0    11  \n",
       "4       []  1647258374000           0               0      3     0    11  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "train_data = pd.read_csv(\"data/train.csv\")\n",
    "eval_data = pd.read_csv(\"data/evaluation.csv\")\n",
    "\n",
    "# Pre-process the traning data\n",
    "label = \"text\"\n",
    "train_data[label] = train_data[label].map(lambda x: str.split(x, sep=\" \"))\n",
    "\n",
    "\n",
    "label = \"mentions\"\n",
    "train_data[label+\"_count\"] = train_data[label].map(lambda x: len(str.split(x)))\n",
    "mentions = set()\n",
    "train_data[label].apply(lambda x: mentions.update(x))\n",
    "print(\"all mentions in train data: \", mentions)\n",
    "\n",
    "\n",
    "label = \"urls\"\n",
    "train_data[label] = train_data[label].map(lambda x: [] if x==\"[]\" else [str.strip(url) for url in str.split(x[1:-1], sep=\",\")])\n",
    "train_data[label+\"_count\"] = train_data[label].map(lambda x: len(x))\n",
    "urls = set()\n",
    "train_data[label].apply(lambda x: None if len(x)<1 else urls.update(x))\n",
    "print(f\"average number of urls in train data: {train_data.urls_count.mean():1.3f}\")\n",
    "print(\"number of distinct urls in eval data: \", len(urls))\n",
    "\n",
    "label = \"verified\"\n",
    "print(\"verified users: \", train_data[\"verified\"].sum())\n",
    "\n",
    "\n",
    "label = \"hashtags\"\n",
    "train_data[label] = train_data[label].map(lambda x: [] if x==\"[]\" else [str.strip(tag) for tag in str.split(x[1:-1], sep=\",\")])\n",
    "train_data[label+\"_count\"] = train_data[label].map(lambda x: len(x))\n",
    "hashtags = set()\n",
    "train_data[label].apply(lambda x: None if len(x)<1 else hashtags.update(x))\n",
    "print(f\"average number of hashtags in train data: {train_data.hashtags_count.mean():1.3f}\")\n",
    "print(\"number of distinct hashtags in train data:\", len(hashtags))\n",
    "\n",
    "\n",
    "label = \"hashtags\"\n",
    "eval_data[label] = eval_data[label].map(lambda x: [] if x==\"[]\" else [str.strip(tag) for tag in str.split(x[1:-1], sep=\",\")])\n",
    "eval_data[label+\"_count\"] = eval_data[label].map(lambda x: len(x))\n",
    "hashtags2 = set()\n",
    "eval_data[label].apply(lambda x: None if not x else hashtags2.update(x))\n",
    "print(f\"average number of hashtags in evaluation data: {eval_data.hashtags_count.mean():1.3f}\")\n",
    "print(\"number of distinct hashtags in evaluation data:\", len(hashtags2))\n",
    "\n",
    "\n",
    "\n",
    "# Treatment of time\n",
    "# The time relative to the election (month and yday), day in the week, \n",
    "# and the hour all affect the number of retweets.\n",
    "train_data[\"time\"] = train_data[\"timestamp\"].map(lambda x: time.gmtime(x//1000))\n",
    "# Only 2380 tweets are not posted in 2022, so we ignore the year\n",
    "# train_data[\"year\"] = train_data[\"timestamp\"].map(lambda x: x.tm_year)\n",
    "train_data[\"month\"] = train_data[\"time\"].map(lambda x: x.tm_mon)\n",
    "train_data[\"wday\"] = train_data[\"time\"].map(lambda x: x.tm_wday)\n",
    "train_data[\"hour\"] = train_data[\"time\"].map(lambda x: x.tm_hour)\n",
    "print(\"number of tweets not posted in 2022 in train data:\", len(train_data[train_data.time.map(lambda x: x.tm_year) < 2022]))\n",
    "\n",
    "# We drop the following data:\n",
    "# @TweetID: useless\n",
    "# @mentions: none of the mentions in the train data are not null\n",
    "# @mentions_count\n",
    "# @urls: we use urls_count instead\n",
    "\n",
    "train_data.drop(labels=[\"TweetID\",\"time\", \"mentions\",\"mentions_count\", \"urls\"], axis=1, inplace=True)\n",
    "train_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we split our training data into trainig and testing set. This way we can estimate the evaluation of our model without uploading to Kaggle and avoid overfitting over our evaluation dataset.\n",
    "# scsplit method is used in order to split our regression data in a stratisfied way and keep a similar distribution of retweet counts between the two sets\n",
    "X_train, X_test, y_train, y_test = scsplit(train_data, train_data['retweets_count'], stratify=train_data['retweets_count'], train_size=0.7, test_size=0.3)\n",
    "\n",
    "# We remove the actual number of retweets from our features since it is the value that we are trying to predict\n",
    "X_train = X_train.drop([\"retweets_count\"], axis=1)\n",
    "X_test = X_test.drop([\"retweets_count\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>favorites_count</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>statuses_count</th>\n",
       "      <th>friends_count</th>\n",
       "      <th>verified</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>urls_count</th>\n",
       "      <th>hashtags_count</th>\n",
       "      <th>month</th>\n",
       "      <th>wday</th>\n",
       "      <th>hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>174230</th>\n",
       "      <td>-0.051518</td>\n",
       "      <td>-0.078478</td>\n",
       "      <td>-0.416483</td>\n",
       "      <td>-0.517543</td>\n",
       "      <td>-0.177268</td>\n",
       "      <td>0.019607</td>\n",
       "      <td>0.926573</td>\n",
       "      <td>2.202999</td>\n",
       "      <td>-0.041798</td>\n",
       "      <td>1.041114</td>\n",
       "      <td>-0.049599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236989</th>\n",
       "      <td>-0.053798</td>\n",
       "      <td>-0.035414</td>\n",
       "      <td>0.569586</td>\n",
       "      <td>3.572957</td>\n",
       "      <td>-0.177268</td>\n",
       "      <td>0.022119</td>\n",
       "      <td>0.926573</td>\n",
       "      <td>0.909360</td>\n",
       "      <td>-0.041798</td>\n",
       "      <td>1.041114</td>\n",
       "      <td>0.693400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10890</th>\n",
       "      <td>-0.053798</td>\n",
       "      <td>-0.079921</td>\n",
       "      <td>-0.411123</td>\n",
       "      <td>-0.502250</td>\n",
       "      <td>-0.177268</td>\n",
       "      <td>0.057583</td>\n",
       "      <td>0.926573</td>\n",
       "      <td>-0.384278</td>\n",
       "      <td>-0.041798</td>\n",
       "      <td>-1.767201</td>\n",
       "      <td>0.693400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        favorites_count  followers_count  statuses_count  friends_count  \\\n",
       "174230        -0.051518        -0.078478       -0.416483      -0.517543   \n",
       "236989        -0.053798        -0.035414        0.569586       3.572957   \n",
       "10890         -0.053798        -0.079921       -0.411123      -0.502250   \n",
       "\n",
       "        verified  timestamp  urls_count  hashtags_count     month      wday  \\\n",
       "174230 -0.177268   0.019607    0.926573        2.202999 -0.041798  1.041114   \n",
       "236989 -0.177268   0.022119    0.926573        0.909360 -0.041798  1.041114   \n",
       "10890  -0.177268   0.057583    0.926573       -0.384278 -0.041798 -1.767201   \n",
       "\n",
       "            hour  \n",
       "174230 -0.049599  \n",
       "236989  0.693400  \n",
       "10890   0.693400  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# standardise the columns of numeric values\n",
    "def standardise(data,list_column):\n",
    "    for name_column in list_column:\n",
    "        # standardize\n",
    "        if data[name_column].std()!=0:\n",
    "            data[name_column]=(data[name_column]-data[name_column].mean())/data[name_column].std()\n",
    "        else:\n",
    "            data[name_column]=(data[name_column]-data[name_column].mean())\n",
    "            \n",
    "standardise(X_train,[\"favorites_count\",\"followers_count\",\"statuses_count\",\"friends_count\",\"urls_count\",\"hashtags_count\",\"verified\",\"timestamp\",\"month\",\"wday\",\"hour\"])\n",
    "standardise(X_test, [\"favorites_count\",\"followers_count\",\"statuses_count\",\"friends_count\",\"urls_count\",\"hashtags_count\",\"verified\",\"timestamp\",\"month\",\"wday\",\"hour\"])\n",
    "\n",
    "# split the table into 2 parts: one with the text and the other with the numbers\n",
    "X_train_num=X_train.drop([\"text\", \"hashtags\"], axis=1)\n",
    "X_test_num=X_test.drop([\"text\", \"hashtags\"], axis=1)\n",
    "\n",
    "X_train_text=X_train[[\"text\", \"hashtags\"]]\n",
    "X_test_text=X_test[[\"text\", \"hashtags\"]]\n",
    "X_train_num.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part is used to train the embedding of the vocabulary, only needed to be run once\n",
    "\n",
    "path_text_dataset='French-Word-Embeddings/Data/data.txt'\n",
    "text_dataset=pd.read_table(path_text_dataset).values\n",
    "text_dataset=[item[0][:-1] for item in text_dataset]\n",
    "\n",
    "TaggedDocument=gensim.models.doc2vec.TaggedDocument\n",
    "\n",
    "def X_text(sentences):\n",
    "    X=[]\n",
    "    for i,text in enumerate(sentences):\n",
    "        words=text.split(\" \")\n",
    "        l=len(words)\n",
    "        words=words[:-2]\n",
    "        document=TaggedDocument(words,tags=[i])\n",
    "        X.append(document)\n",
    "    return X\n",
    "\n",
    "X_documents=X_text(text_dataset)\n",
    "\n",
    "def train_text(text_train,size=10,epochs=10):\n",
    "    model=Doc2Vec(text_train,min_count=1,window=3,vector_size=size,sample=1e-3,negative=5,epochs=epochs)\n",
    "    model.train(text_train,total_examples=model.corpus_count,epochs=model.epochs)\n",
    "    return model\n",
    "\n",
    "if not os.path.exists('./WE_models'):\n",
    "    os.mkdir('./WE_models')\n",
    "\n",
    "# run only once this part  \n",
    "# model_text128=train_text(X_documents, size=128)\n",
    "# model_text128.save('WE_models/d2v_128D')\n",
    "# model_text64=train_text(X_documents, size=64)\n",
    "# model_text64.save('WE_models/d2v_64D')\n",
    "# model_text32=train_text(X_documents, size=32)\n",
    "# model_text32.save('WE_models/d2v_32D')\n",
    "# model_text10=train_text(X_documents, size=10)\n",
    "# model_text10.save('WE_models/d2v_10D')\n",
    "# model_text5=train_text(X_documents, size=5)\n",
    "# model_text5.save('WE_models/d2v_5D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we load the embedding model:\n",
    "model_text128 = Doc2Vec.load('WE_models/d2v_128D')\n",
    "# model_text32 = Doc2Vec.load('WE_models/d2v_32D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tranform the textual part to vector\n",
    "\n",
    "def text2vec(text_train, model, alpha=0.05, min_alpha=0.025, epochs=10, tags=True):\n",
    "    list_text=[]\n",
    "    for sentence in text_train:\n",
    "        vec = model.infer_vector(doc_words=sentence, alpha=alpha, min_alpha=min_alpha, epochs=epochs).tolist()\n",
    "        list_text.append(vec)\n",
    "    return np.array(list_text)\n",
    "\n",
    "# text to vector\n",
    "X_train_tweet_vec = text2vec(X_train_text[\"text\"], model=model_text128)\n",
    "train_text_tensor=torch.Tensor(X_train_tweet_vec)\n",
    "X_test_tweet_vec = text2vec(X_test_text[\"text\"], model=model_text128)\n",
    "test_text_tensor=torch.Tensor(X_test_tweet_vec)\n",
    "\n",
    "# hashtags to vector\n",
    "X_train_tags_vec = text2vec(X_train_text[\"hashtags\"], model=model_text128)\n",
    "train_tags_tensor=torch.Tensor(X_train_tags_vec)\n",
    "X_test_tags_vec = text2vec(X_test_text[\"hashtags\"], model=model_text128)\n",
    "test_tags_tensor=torch.Tensor(X_test_tags_vec)\n",
    "\n",
    "# combine the two vectors\n",
    "X_train_text_vec = np.concatenate((X_train_tweet_vec, X_train_tags_vec), axis=1)\n",
    "X_test_text_vec = np.concatenate((X_test_tweet_vec, X_test_tags_vec), axis=1)\n",
    "\n",
    "X_train_text_tensor = torch.Tensor(X_train_text_vec)\n",
    "X_test_text_tensor = torch.Tensor(X_test_text_vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([247778, 11]) torch.Size([106191, 11])\n",
      "torch.Size([247778, 256]) torch.Size([106191, 256])\n",
      "torch.Size([247778, 267]) torch.Size([106191, 267])\n",
      "torch.Size([247778, 1]) torch.Size([106191, 1])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train_num_vec=X_train_num.values\n",
    "X_train_num_tensor=torch.Tensor(X_train_num_vec)\n",
    "X_test_num_vec=X_test_num.values\n",
    "X_test_num_tensor=torch.Tensor(X_test_num_vec)\n",
    "\n",
    "X_train_vec=np.concatenate((X_train_num_vec, X_train_text_vec), axis=1)\n",
    "X_test_vec=np.concatenate((X_test_num_vec, X_test_text_vec), axis=1)\n",
    "\n",
    "X_train_tensor=torch.Tensor(X_train_vec)\n",
    "X_test_tensor=torch.Tensor(X_test_vec)\n",
    "\n",
    "y_train_vec=y_train.values\n",
    "y_test_vec=y_test.values\n",
    "y_train_tensor=torch.Tensor(y_train_vec).reshape((-1,1))\n",
    "y_test_tensor=torch.Tensor(y_test_vec).reshape((-1,1))\n",
    "\n",
    "print(X_train_num_tensor.shape,X_test_num_tensor.shape)\n",
    "print(X_train_text_tensor.shape, X_test_text_tensor.shape)\n",
    "print(X_train_tensor.shape, X_test_tensor.shape)\n",
    "print(y_train_tensor.shape, y_test_tensor.shape)\n",
    "\n",
    "# dataset_num_XY = torch.utils.data.TensorDataset(X_train_num_tensor.to(device), y_train_tensor.to(device))\n",
    "# testset_num_XY = torch.utils.data.TensorDataset(X_test_num_tensor.to(device), y_test_tensor.to(device))\n",
    "\n",
    "# dataset_tot_XY = torch.utils.data.TensorDataset(X_train_tensor.to(device), y_train_tensor.to(device))\n",
    "# testset_tot_XY = torch.utils.data.TensorDataset(X_test_tensor.to(device), y_test_tensor.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAE(pred,real):\n",
    "    N=len(pred)\n",
    "    sum=0\n",
    "    for i in range(N):\n",
    "        sum+=abs(pred[i]-real[i])\n",
    "    return sum/N\n",
    "\n",
    "def ACC(pred,real):\n",
    "    N=len(pred)\n",
    "    sum=0\n",
    "    for i in range(N):\n",
    "        if pred[i]==real[i]:\n",
    "            sum+=1\n",
    "    return sum/N\n",
    "\n",
    "def model_result(model,X_train,y_train,X_test,y_test):\n",
    "    model.fit(X_train,y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred = np.array([int(value) if value >= 0 else 0 for value in y_pred])\n",
    "    loss=MAE(y_pred,y_test)\n",
    "    acc=ACC(y_pred,y_test)\n",
    "    return loss,acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1177,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_list=[0.1*i for i in range(30)]\n",
    "loss_Lasso_tot=[]\n",
    "acc_Lasso_tot=[]\n",
    "for alpha in alpha_list:    \n",
    "    model_Lasso=Lasso(alpha=alpha)\n",
    "    loss_Lasso,acc_Lasso=model_result(model_Lasso,X_train_vec,y_train_vec,X_test_vec,y_test_vec)\n",
    "    loss_Lasso_tot.append(loss_Lasso)\n",
    "    acc_Lasso_tot.append(acc_Lasso)\n",
    "    \n",
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "ax.scatter(alpha_list,loss_Lasso_tot)\n",
    "ax.plot(alpha_list,loss_Lasso_tot) \n",
    "plt.title(\"Loss for Lasso model\")\n",
    "plt.show()\n",
    "\n",
    "model_Lasso=Lasso(alpha=0.1*np.argmin(np.array(loss_Lasso_tot)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.414658492715954 0.1452100460491002\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9.414639658728142, 0.14520062905519301)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_Lasso,acc_Lasso=model_result(model_Lasso,X_train_vec,y_train_vec,X_test_vec,y_test_vec)\n",
    "print(loss_Lasso,acc_Lasso)\n",
    "\n",
    "loss_Lasso_num,acc_Lasso_num=model_result(model_Lasso,X_train_num_vec,y_train_vec,X_test_num_vec,y_test_vec)\n",
    "loss_Lasso_num,acc_Lasso_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "alpha_list=[i for i in range(7,15)]\n",
    "loss_Knn_tot=[]\n",
    "acc_Knn_tot=[]\n",
    "for alpha in alpha_list:    \n",
    "    print(alpha)\n",
    "    model_Knn=KNeighborsRegressor(n_neighbors=alpha)\n",
    "    loss_Knn,acc_Knn=model_result(model_Knn,X_train_vec,y_train_vec,X_test_vec,y_test_vec)\n",
    "    loss_Knn_tot.append(loss_Knn)\n",
    "    acc_Knn_tot.append(acc_Knn)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "ax.scatter(alpha_list,loss_Knn_tot)\n",
    "ax.plot(alpha_list,loss_Knn_tot) \n",
    "plt.title(\"Loss for Knn model\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.042338804606793 0.28482639771732066\n",
      "9.099029107928168 0.34160145398385927\n"
     ]
    }
   ],
   "source": [
    "model_Knn=KNeighborsRegressor(n_neighbors=np.argmin(np.array(loss_Knn_tot))+7)\n",
    "loss_Knn,acc_Knn=model_result(model_Knn,X_train_vec,y_train_vec,X_test_vec,y_test_vec)\n",
    "print(loss_Knn,acc_Knn)\n",
    "loss_Knn_num,acc_Knn_num=model_result(model_Knn,X_train_num_vec,y_train_vec,X_test_num_vec,y_test_vec)\n",
    "print(loss_Knn_num,acc_Knn_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAA2DElEQVR4nO3deXzU9bXw8c/JvpIwWYCELQENIMhO0LqwqNQdbW1tta22T722antb663e9ra3tY/1lnrbPo/30dq61ELtYqmobQVEwRUQJICERbaQhWyEJCRknTnPHzPRGBMygcn8Zibn/XrlZfJb5nfmZ3L4zvl9F1FVjDHGRK4opwMwxhgzuCzRG2NMhLNEb4wxEc4SvTHGRDhL9MYYE+Es0RtjTISzRG8ikogkisgLItIgIn9xOp5gEZHxIqIiEuPHsbeIyBvBiMs4yxK9GVQiclhELnHg0p8GRgAZqnrDmb6YiCwQEY+INInICRHZKyK39jhGRaTZd0yTiNSf6XWNCQRL9CZSjQP2qWrnQE88RWu4QlVTgGHAt4DfiEhBj2Omq2qK7yt9oNc2ZjBYojeOEJF4EfmliFT4vn4pIvG+fZki8qKI1ItInYi8LiJRvn3fFZHybq3qxb289o+AHwCf9bWsvyIiUSLyfREpEZFqEXlaRNJ8x3eVO74iIkeAV04Vu3r9A6gDzj2N936LiLwpIr/wvceDInK+b3upL74vdTs+zRdvjS/+73e7H9Ei8nMRqRWRg8CVPa6VJiKPi8hR3337iYhEDzRmE976reMZM0i+B8wHZgAKrAK+D/wHcDdQBmT5jp0PqK/1fCcwV1UrRGQ88LGkpao/FBEFJqrqzQAi8mXgFmAhUA08DTwMfKHbqRcDkwHPqQL3JdmrgExg/8De9gcKgd8CGcCPgD8CLwATfXH8VUT+qqpNwP8F0oB83/FrgKPA48BXfbHMBJqBv/a4zlN43+9EIBl4ESgFfn2acZtwpKr2ZV+D9gUcBi7pZfsB4IpuPy8BDvu+/zHexD+xxzkT8SatS4DYfq77n8Dybj+vA77e7ecCoANvY2c83n9s8k/xegvw/gNQD7QBbuBfexyjQKPvmHrg//TxWrcA73f7eZrv3BHdth3D+49gNNAOTOm271+A9b7vXwFu77bvMt9rxeB9RtEGJHbb/zng1W5xvOH074h9Df6XlW6MU3KAkm4/l/i2ASzD21Je4ytr3AugqvuBf8WbxKtF5I8ikoN/erteVzLsUtrPa1Sot+4+DPg/wKJejpmlqum+r2+c4rWqun3fAqCqPbel4P3UENtL7Lm+73N6xN39uHG+c4/6SkT1eFvy2aeIy0QgS/TGKRV4E1GXsb5tqOoJVb1bVfOBa4Bvd9XiVfUPqnqB71wF/usMrtfJRxOuX1O5qmob8F1gmogs9fP6p6sW7yePnrGX+74/Cozpsa9LKd4WfWa3f3yGqeo5gxmwCT2W6E0wxIpIQrevGOAZ4PsikiUimXgfni4HEJGrRGSiiAjQgLdM4hGRAhFZ5Hto24q31XvKeno3zwDfEpE8EUkBHgD+pKfRKwdAVduBh3xxDxpVdQN/Bv63iKSKyDjg2/julW/fN0RktIgMB+7tdu5RvPX8h0RkmO+B9AQRuXgwYzahxxK9CYZ/4E3KXV//CfwE2ALsAHYC7/q2AZwFvAw0AW8D/09VXwXigQfxtnIr8ZYg7vMzhieA3wOvAYfw/kNx15m9LZ4AxorI1Wf4Ov25C++D1oPAG8AffNcG+A2wGtiO9x6u7HHuF4E4oBg4DjwLjBrkeE2IEVVbeMQYYyKZteiNMSbCWaI3xpgIZ4neGGMinCV6Y4yJcCE5BUJmZqaOHz/e6TCMMSZsbN26tVZVs3rbF5KJfvz48WzZssXpMIwxJmyISElf+6x0Y4wxEc4SvTHGRDhL9MYYE+Es0RtjTISzRG+MMREuJHvdGGNMMDy3rZxlq/dSUd9CTnoi9ywpYOnM3P5PDDOW6I0xQ9Jz28q5b+VOWjrcAJTXt3Dfyp0AEZfsrXRjjBmSlq3e+0GS79LS4WbZ6r0ORTR4LNEbY4akivqWAW0PZ5bojTFDUk564oC2hzNL9MaYIemeJQXERX80BSbGRnPPkgKHIho8luiNMUPS0pm5zB43HPH9HCXwwHVTI+5BLFiiN8YMUW2dboqPNrJ0Zi4PXj8Nj8K00elOhzUoLNEbY4ak1/bV0tDSwTUzcijMzwBg06FjDkc1OCzRG2OGpFVF5biS47hgYibjM5LITo1n08E6p8MaFH4lehFJF5FnRWSPiOwWkfN67E8TkRdEZLuI7BKRW7vtGysia3znFYvI+AC/B2OMGZDmtk5e3l3FldNGERsdhYhQmJ/BpkPHUFWnwws4f1v0vwJeUtVJwHRgd4/9dwDFqjodWAA8JCJxvn1PA8tUdTIwD6g+46iNMeYMrC2uorXDw7Uzcj7YVpjnoqqxjZJjJx2MbHD0m+hFJA24CHgcQFXbVbW+x2EKpIqIAClAHdApIlOAGFVd6zu3SVUj7y4aY8LKqqJyctMTmTV2+Afb5ue7gMis0/vTos8DaoAnRWSbiPxWRJJ7HPMwMBmoAHYC31RVD3A2UC8iK33nLhOR6EC+AWOMGYhjTW289n4t18zIISpKPtg+ISuFzJS4iKzT+5PoY4BZwCOqOhNoBu7tccwSoAjIAWYAD4vIMN+5FwLfAeYC+cAtvV1ERG4TkS0isqWmpmbAb8QYY/zxj/cqcXv0I2UbABFhXp6LTYeGZqIvA8pUdZPv52fxJv7ubgVWqtd+4BAwyXdukaoeVNVO4LlezgVAVR9T1TmqOicrq9eFzI0x5ow9X1ROwYhUJo0c9rF9hXkZlNe3UFoXWRXmfhO9qlYCpSLSNS54MVDc47Ajvu2IyAigADgIvAOki0hX5l7Uy7nGGBMUZcdP8s7h41zTozXfpfCDOn1kter97XVzF7BCRHbgLc08ICK3i8jtvv33A+eLyE5gHfBdVa1VVTfess063z4BfhPQd2CMMX56YftRAK6Z3nuiPzs7lfSkWDYdjKwHsn4tPKKqRcCcHpsf7ba/Arisj3PXAueeZnzGGBMwq4rKmT1uOGNcSb3uj4oS5o6PvDq9jYw1xgwJeytPsKfyxMcewvZUmOfiSN1JjjZEzrz0luiNMUPC89vLiY4Srpg26pTHze+a9yaCullaojfGRDxVZVVRBRdMzCQzJf6Ux04eNYzUhJiIGjhlid4YE/HePVJP2fGWfss2ANERWKe3RG+MiXjPF5UTHxPFZeeM9Ov4wjwXB2uaqT7ROsiRBYclemNMROt0e3hxx1EumTyClHi/Ohp+MD/95ghp1VuiN8ZEtDcPHONYc3ufg6R6MzVnGMlx0RHzQNYSvTEmoq0qKic1IYYFBf5PrRITHcXs8a6IeSBrid4YE7FaO9ysfq+SK6aOIj5mYBPnFua52FfVRF1z+yBFFzyW6I0xEWvd7mqa291+9bbpqWt++s0R0Kq3RG+MiVirisrJTo3/4OHqQEzLTSchNoqNEVCnt0RvjIlIDSc7WL+3hqun5xDdbYERf8XFRDF73PCI6E9vid4YE5Fe2nWUdrfntMo2XQrzMthT2UjDyY4ARhZ8luiNMRFpVVEFeZnJTMtNO+3XKMxzoQqbD4d3q94SvTEm4lQ1tvL2wWNcMz0HkYGXbbpMH5NOXExU2M9Pb4neGBNxXtxxFFUGNEiqNwmx0cwckx72dXpL9MaYiPN8UTnTctOYkJVyxq9VmJ/BrooGGlvDt05vid4YE1EO1TazvazhjB7Cdjc/z4VHYevh4wF5PSdYojfGRJTniyoQgavODUyinzl2OLHRwsYwHjhlid4YEzFUlVXby5mfl8HItISAvGZiXDTTR6eH9QRnluiNMRFjV0UjB2uaA1a26VKY72JneQPNbZ0Bfd1gsURvjIkYq4rKiY0WLp966nVhB6owLwO3R9laEp51ekv0xpiI4PYoz2+vYEFBNmlJsQF97dnjhhMdJWE7bbElemNMRNh8qI6qxraAl20AkuNjmJabFrZ1er8SvYiki8izIrJHRHaLyHk99qeJyAsisl1EdonIrT32DxORMhF5OJDBG2NMl+e3l5McF83iSSMG5fUL811sL6unpd09KK8/mPxt0f8KeElVJwHTgd099t8BFKvqdGAB8JCIxHXbfz/w2hnGaowxvWrrdPOPnZVcds5IEuMGtsCIv+bnZdDhVrYdCb86fb+JXkTSgIuAxwFUtV1V63scpkCqeCeVSAHqgE7f+bOBEcCawIVtjDEfem1fLQ0tHWc85cGpzB4/nCiBjWE4HYI/Lfo8oAZ4UkS2ichvRSS5xzEPA5OBCmAn8E1V9YhIFPAQ8J3+LiIit4nIFhHZUlNTM7B3YYwZ0lYVleNKjuOCiZmDdo1hCbFMyRkWlitO+ZPoY4BZwCOqOhNoBu7tccwSoAjIAWYAD4vIMODrwD9Utay/i6jqY6o6R1XnZGX5v4ivMWZoa2rr5OXdVVw5bRSx0YPbv6QwL4NtR+pp6wyvOr0/d6UMKFPVTb6fn8Wb+Lu7FVipXvuBQ8Ak4DzgThE5DPwc+KKIPBiQyI0xBlhbXElrx5ktMOKvwjwXbZ0etpc2DPq1AqnfRK+qlUCpiBT4Ni0GinscdsS3HREZARQAB1X1JlUdq6rj8ZZvnlbVnp8GjDHmtK0qqiA3PZFZY4cP+rXm5bkQIezmp/f3c85dwAoR2YG3NPOAiNwuIrf79t8PnC8iO4F1wHdVtTbg0RpjTDfHmtp4/f1arpmRQ9RprAs7UOlJcRSMSA27+elj/DlIVYuAOT02P9ptfwVwWT+v8RTw1ICiM8aYU/jHzqO4PRqUsk2X+fkZ/OmdUjrcnkF/JhAo4RGlMcb0YlVRBQUjUpk0cljQrlmY56Klw82OsvCp01uiN8aEpdK6k2wpOT6ofed7My/PBRBW895YojfGhKUXdlQAcM304Cb6jJR4zspOCat5byzRG2PC0vNFFcweN5wxrqSgX7sw38WWw3V0uj1Bv/bpsERvjAk7eytPsKfyRFAfwnZXmJdBc7ubXRWNjlx/oCzRG2PCzvPby4mOEq6YFtgFRvxVmB9edXpL9MaYsKKqrCqq4IKJmWSmxDsSQ3ZqAvmZyWFTp7dEb4wJK+8eqafseItjZZsuhfkuNh+uw+1RR+PwhyV6Y0xYeb6onPiYKC47Z6SjcRTmZXCitZPdR0O/Tm+J3hgTNjrdHl7ccZRLpowgJd6vgf2D5sM6feiXbyzRG2PCxpsHjnGsuZ1rg9x3vjej0hIZ60oKiwnOLNEbY8LGqqJyhiXEcHFBaKxZUZjnrdN7QrxOb4neGBMWWjvcrH6vksunjiI+ZnDWhR2owvwM6k92sK/6hNOhnJIlemNMWFi3u5rmdrfjvW26K+ya9ybEu1laojfGhIVVReVkp8ZTmJ/hdCgfGONKIjc9MeQHTlmiN8aEvIaTHazfW8PV03OIDsICIwNRmOdi86E6VEO3Tm+J3hgT8l7adZR2d3DWhR2ownwXtU3tHKhpcjqUPlmiN8aEvFVFFeRlJjMtN83pUD6mMM9bStoYwnV6S/TGmJBW1djK2wePcc30HERCq2wDMC4jiRHD4tkcwgOnLNEbY0LaC9srUCXoK0n5S0QozMtg06FjIVunt0RvjAlpz2+vYFpuGhOyUpwOpU+F+S6qGtsoOXbS6VB6ZYneGBOyDtY0saOsISQfwnbXVacP1W6WluiNMSHr+e0ViMBV54Z2op+QlUxmSlzIDpyyRG+MCUmqyvNFFczPy2BkWoLT4ZySiDAvzxWyM1n6lehFJF1EnhWRPSKyW0TO67E/TUReEJHtIrJLRG71bZ8hIm/7tu0Qkc8OxpswxkSe98obOVjbHPJlmy6FeRmU17dQWhd6dXp/J3T+FfCSqn5aROKAnsuu3wEUq+rVIpIF7BWRFcBJ4Iuq+r6I5ABbRWS1qtYH6g0YYyLTqqJyYqOFy6c6sy7sQHWfn36Mq2eKdFa/LXoRSQMuAh4HUNX2XhK1Aqni7eSaAtQBnaq6T1Xf951XAVQDoTG/qDEmZLk9ygs7KlhQkE1aUqzT4fjl7OxU0pNiQ3J+en9KN3lADfCkiGwTkd+KSHKPYx4GJgMVwE7gm6rq6X6AiMwD4oADvV1ERG4TkS0isqWmpmag78MYE0E2H6qjqrEtbMo2AFFRwrzxoVmn9yfRxwCzgEdUdSbQDNzb45glQBGQA8wAHhaRYV07RWQU8Hvg1p7/AHRR1cdUdY6qzsnKska/MUPZ89vLSY6LZvGkEU6HMiCF+RkcqTvJ0YYWp0P5CH8SfRlQpqqbfD8/izfxd3crsFK99gOHgEkAvoT/d+B7qroxMGEbYyJVW6ebf+ysZMk5I0mMC40FRvwVqvPT95voVbUSKBWRAt+mxUBxj8OO+LYjIiOAAuCg78Ht34CnVfXZgEVtjIlYr+2rpaGlI2SnPDiVyaOGkZoQE3IDp/ztdXMXsMKXuA8Ct4rI7QCq+ihwP/CUiOwEBPiuqtaKyM14H+RmiMgtvte6RVWLAvgeTA/PbStn2eq9VNS3kJOeyD1LClg6M9fpsIzxy6qiclzJcXxiYqbToQxYdFedPsRa9H4lel9intNj86Pd9lcAl/Vy3nJg+RnEZwbouW3l3LdyJy0dbgDK61u4b+VOAEv2JuQ1tXXy8u4qbpg9htjo8BzPWZjvYt2eaqobW8keFhoDvcLzTpo+LVu994Mk36Wlw81P/l6MO8RXqjdmbXElrR2hucCIvz6c9yZ0WvWW6CNMRX3vT/trm9qZ/ZO1fOOZbfxtWxnHmtqCHJkx/VtVVEFueiKzxg53OpTTdk7OMFLiQ6tO72+N3oSJnPREyntJ9q6kOBZNzmb93poPJoqaPjqdhQXZLJyUxdScNKJCbC1OM7Qca2rj9fdrue2i/LD+XYyJjmL2uOEhVae3RB9h7llSwL89u4N294fDFRJjo/nB1VNYOjMXj0fZVdHIK3uqeXVvNb9ct49fvLyPzJQ4Lj7bm/QvPCuLtMTwGI1oIsc/dh7F7dGwLtt0Kcx38bOX9nKsqY2MlHinw7FEH2mWzsxlVVE5r+71ji7O7dHrJipKmDY6jWmj0/jmJWdxrKmN196v4dU9Nby8u4q/vltGdJQwe9zwD1r7BSNSQ3IJNxNZVhVVUDAilUkjh/V/cIjrqtNvPlTH5dOcn6vHEn2EUVUO1jZz8dlZ/O7L8/o9PiMlnutmjua6maPpdHvYXlbPq3tqeHVvNf/10h7+66U9jEpLYEFBNgsLsvjExEyS4+3XxgRWad1JtpQc554lBf0fHAbOHZ1GYmw0myzRm8HwfnUTJcdO8tUL8wd8rre26GL2OBffWVJAVWMrG/bW8Mqeal7YXsEzm48QFx3FvDwXCwqyWDgpm/zMZGvtmzP2wo4KAK6ZHv5lG4BYX51+Y4hMcGaJPsKsLa4C4NIpZz5HyIhhCXxm7hg+M3cM7Z0etpTUsX5vDa/uqeYnf9/NT/6+m7GuJBYWZLFgUjbn5WeQEPvhkHUbuGX89XxRBbPHDQ+56X3PRGGei/9+eR/1J9tJT4pzNBZL9BFmTXEV08ekMyLAAzXiYqI4f0Im50/I5N+vmExp3UnW76th/Z5q/rSllN+9XUJCrPeYhQVZtHd6+PmafTZwy/RrT2UjeypP8ONrz3E6lIAqzM9A1Vunv+yckY7GYok+glQ1trK9tD4odc4xriS+MH8cX5g/jtYONxsPHmO9r8zzyp7qXs9p6XCzbPVeS/TmI54vqiA6SrgiBGrZgTR9TBrxMVGW6E1gBbJsMxAJsdEsKMhmQUE2P7x6Codqm1n00IZej+1rQJcZmlSVVUUVXDAxk8wQ6IYYSPEx0cwcmx4SI2RtZGwEWVNcxfiMJM7KTnEsBhEhPyuF3PTEXvfn9LHdDD3PbStn7v9+mfL6FnaU1fPctnKnQwq4wrwMdlU00Nja4WgclugjxInWDt4+UMulU0aERC+Ye5YUkBj78bnEb54/1oFoTKjpmnyvtqkdgOMnO7hv5c6IS/aF+S48ClsPH3c0Dkv0EWLDvho63MqlU5ytBXZZOjOXn14/jdz0RAQYkRrPsIQYnn67JORW3zHB19fke8tW73UoosExa+xw4qKj2OjwvDdWo48Qa3ZV4UqOY/a40JkMaunM3I88eN1V0cBnf72RW554hz//y3lhs+izCbze5mOCyHuGkxAbzfQxaY7Pe2Mt+gjQ4fbw6t5qFk/KJjqEJ4M6JyeNx74wm4O1TXz16S209mjRmch3uLaZ23+/tc/9kfgMpzAvg53lDTS3dToWgyX6CLDpYB0nWjsd78Llj/MnZvLQZ2aw+XAd//rHIpsjf4hoONnBT14s5tJfbOC192u4fOpIEmI+mn4SY6MjZgqE7grzXbg9ytYS5+r0VrqJAGuKK0mIjeKCMFl67ZrpOdScaOP+F4v5z+d38eNrzwmJB8gm8DrcHlZsLOGX696noaWDz8wew92XnU32sIQhM3J69rjhxEQJmw4d46KzsxyJwRJ9mFNVXi6u4sKzskiM+3gvl1D1lQvyqG5s5devHWRkWgJ3LJzodEgmgFSVl3dX89N/7OZgbTPnT8jg+1dOYUrOhzNT9nyGE6mS4mKYmutsnd4SfZjbVdFIRUMr37r0bKdDGbDvfnIS1SfaWLZ6L1mp8XxmzhinQzIBsKuigZ+8uJu3Dx4jPyuZx780h0WTsof0p7bCfBdPvHGIlna3Iw0yS/Rhbs2uSqIEFk8O7mjYQIiKEv7rU+dS29TGfSt3kpkSx6JJ4fc+jFdVYys/X72XZ98tIz0xlh9dcw6fLxwbtot8B9L8vAx+veEg244c53wHSqz2fyDMrSmuYs44F65kZ2fHO11xMVE8cvNsJo9K5esr3mXbEWcHlpiBO9neya9efp8Fy9bzXFE5X70wn/X3LORL54+3JO8zZ/xwogQ2OjQdgv1fCGOldSfZU3mCy84J71ZwSnwMT94yj+zUBL781DscqGlyOiTjB49H+evWMhb9fAO/eHkfCydl8fK3L+bfr5hsS1H2kJoQyzk5aWxyaH56S/RhbI1Dk5gNhqzUeJ7+8jyiRPjSE5upbmx1OiRzChsPHuOa/3mDu/+ynRHD4vnL7efx/26azbiMZKdDC1mFeS62ldY7Mn7Er0QvIuki8qyI7BGR3SJyXo/9aSLygohsF5FdInJrt31fEpH3fV9fCvQbGMrWFldy9oiUiPnjGp+ZzJO3zqWuuZ0vPfmO4xNBmY87VNvMbU9v4cbHNlLX1M4vPzuDv339E8wd73I6tJBXmJ9Be6eH7aX1Qb+2vy36XwEvqeokYDqwu8f+O4BiVZ0OLAAeEpE4EXEBPwQKgXnAD0UkdMboh7Hjze3eea5DZG6bQDl3dDqP3Dyb96tOcPvvt9LWaaNnQ0H9yXZ+/EIxl/73Bt7cX8s9Swp45TsLWDozl6gQHo0dSuaNdyGCI9MW95voRSQNuAh4HEBV21W1vsdhCqSKt/9UClAHdAJLgLWqWqeqx4G1wCcDF/7Q9cqeajwaGWWbni4+O4ufffpc3jpwjLv/vB2PjZ51THunhyfeOMTFy9bz1FuHuGHOaF69ZwF3LJz4kWUjTf/SkmKZNHIYmxyY4Myf7pV5QA3wpIhMB7YC31TV5m7HPAw8D1QAqcBnVdUjIrlAabfjyoBeR0iIyG3AbQBjx9pUtv1ZW1zFyGEJTMtNczqUQXH9rNFUn2jjwX/uITs1gf+4avKQ7ocdbKrK2uIqfvrPPRyqbeaCiZl878rJTB41rP+TTZ8K81z88Z0jtHd6iIsJ3iNSf64UA8wCHlHVmUAzcG+PY5YARUAOMAN4WEQG9Buhqo+p6hxVnZOV5cww4XDR2uFmw74aLpmSHdEfm//lonxu/cR4nnjzEL95/aDT4QwZ75U3cONjG7nt91uJjhKevGUuv//KPEvyATA/30Vrh4ed5fVBva4/LfoyoExVN/l+fpaPJ/pbgQdVVYH9InIImASU463ZdxkNrD+TgA28ub+Wlg53yMw9P1hEhP+4cgrVJ9p44B97yEqN57qZo50OK2JVNrSybPVeVm4rY3hSHPcvncrn5o4hxvrCB8y8vAwANh6sY/a44D3A7jfRq2qliJSKSIGq7gUWA8U9Djvi2/66iIwACoCDwH7ggW4PYC8D7gtY9EPUml1VpMbHcF5+htOhDLqoKOG/PzOduqZ27vnLDjKS4x2bGCpS9JxM7BuLJ1JR38pjrx3E7VFuuyifOxZOZFiC9YUPNFdyHGePSGHToTruWBi86/o7BcJdwAoRicObwG8VkdsBVPVR4H7gKRHZCQjwXVWtBRCR+4F3fK/zY1V1fqXcMOb2KOv2VHFxQVZQa3xOio+J5tdfnM1nHn2b25dv5U+3nce00ZH5bGKwdS3h17W6U3l9C9/9604Arjx3FPd+chJjXElOhhjxCvMyWPluGZ1uT9A+Lfl1FVUt8tXPz1XVpap6XFUf9SV5VLVCVS9T1WmqOlVVl3c79wlVnej7enKw3shQUVR6nNqm9ojsbXMqwxJi+d2X5zE8KY5bn9pMybHm/k8yH9PbEn4AmSlx/M/nZ1mSD4LCfBfN7W7eq2gM2jWHRpMwgqzZVUVstLBwUrbToQTdiGEJPP2Vebg9yhef2ExtU5vTIYWdvpbqO+ZbpNsMvnl53tr85iB2s7REH2bWFlcxPz9jyNZPJ2Sl8Pgtc6lqbOXWJ99xdHm2cNTXUn2RuIRfqMpOTSA/Kzmo89Nbog8j+6ubOFjbPOTKNj3NGjuc//n8LIqPNvK1Fe/S4fY4HVLY+NYlZ31sW6Qu4RfKCvMy2Hy4LmhLaVqiDyNriisBuCQM554PtMWTR/DAdVN5bV8N3312B96evaY/cb7RrBnJcQiQm57IT6+fNiRWegol8/NdnGjtZPfR4NTpbeGRMLK2uIppuWn2Mdvns3PHUt3YxkNr95E9LIF7L5/kdEghb/nbJYx1JbH+OwsierBdqCv09affdKiOqUEY3W4t+jBR3djKtiP1XDbEyzY93bloIjfPH8ujGw7wxBuHnA4npO2tPMHmw3V8vnCsJXmHjUxLYFxGUtDmp7cWfZh4eXc1AJeG+SIjgSYi/OiaqdSeaOf+vxeTlRrP1dNznA4rJK3YVEJcdBQ3zLbRxaGgMM/FmuIqPB4d9H94rUUfJtYWVzLGlUjBiFSnQwk50VHCL2+cwZxxw7n7z9t560Ct0yGFnOa2Tla+W84V00aSkRLvdDgGb/mm/mQH+6pPDPq1LNGHgaa2Tt7cf4zLpoy0GRz7kBAbzW+/OJfxmUn8y9NbKQ7iYJRwsKqogqa2Tm6eP87pUIxPYb63P30wullaog8Dr+2rod3tGfLdKvuTlhTLU7fOIyUhhlue3Exp3UmnQwoJqsryjSVMGpnK7HG27k+oGD08idz0xKDMT2+JPgysLa4iPSmWOfZH2q+c9ER+9+V5tHa4+dKTm6lrthGf20rrKT7ayE3zx9knwhBTmO9i86G6Qe8ebIk+xHW4PazbXcXiSSNsulg/nT0ilcdvmUv58Ra+8rt3aGkf2ssRLt9YQnJcNNdZX/mQMz8vg9qmdg7UNA3qdazXTYh751Adja2dVrYZoLnjXfzqxpl8fcVWPv3oWxxvbudoQys56Yncs6RgyAwQOt7czos7jvKZOaNJibc/91DTVaffeLCOidmD19HCmoghbk1xFfExUVx0dqbToYSdT04dyadmjWZXRSMVDa0o3ml571u5k+e2lTsdXlA8u7WM9k6PPYQNUWNdSYwcljDoC4Zbog9hXet2XnhWJklx1ho7HW8d+PiDrpYON8tW73UgmuDyeJQVm0qYM244k0baMoChSESYl+di08Fjg1qnt0QfwoqPNlJe32JlmzPQ17S8fW2PJG8eqOXwsZPWmg9xhfkuqk+0cfjY4PUSs0QfwtbsqkLEO4GXOT1DeVre5RtLcCXHcfm0yF5bONx9MO/NIE6HYIk+hK0trmL22OFk2kjG03bPkgISfTM2domPiYr4aXkrG1p5eXc1N8wZTXxMdP8nGMdMyEomMyV+UOv0luhDVNnxkxQfbbSyzRlaOjOXn14/jdxuLfg544ZHfK+bZzYfwaPKTfOsbBPqRITCQa7TW6IPUWuLqwC47Bz72H2mls7M5c17F3H4wSv5fOFY3jl8PKJr9B1uD3985wgXnZXF2AxbAzYcFOa7qGhopez44PxeWqIPUWuLq5iYnUJeZrLToUSUry+YgKI8sv6A06EMmnW7q6hqbLOHsGGkudW7JOaFP3uVTzz4SsC7/1qiD0ENJzvYdKjOyjaDYPTwJG6YM4Y/vVPK0YbIbNUv33iEnLQEFg3BBeTD0XPbyvnVuvc/+HkwxnpYog9Br+ytwu1RW2RkkERyq/5QbTNv7K/lc/PGEm2Li4SFZav30tr50XWPAz3WwxJ9CFpbXEV2ajzTR6c7HUpEGj08iU/PHsMfN0deq37FxhJiooTPzhvjdCjGT8EY6+FXoheRdBF5VkT2iMhuETmvx/57RKTI9/WeiLhFxOXb9y0R2eXb/oyIJAQs+gjU2uFm/d4aFk8eYcu9DaI7Fk7Ao5HVqm/tcPOXrWUsOWck2an2ZxYugjHWw98W/a+Al1R1EjAd2N19p6ouU9UZqjoDuA/YoKp1IpILfAOYo6pTgWjgxoBFH4HePnCMk+1uLrMlAwdVV60+klr1L+44SkNLBzfNH+t0KGYAehvrkRgbHdCxHv0mehFJAy4CHgdQ1XZVrT/FKZ8Dnun2cwyQKCIxQBJQcdrRDgFriqtIjovm/AkZTocS8b6+wNuqfzRCWvXLN5YwISuZ8/LtdyecdB/rIUBueiI/vX5aQMd6+DNTVh5QAzwpItOBrcA3VbW554EikgR8ErgTQFXLReTnwBGgBVijqmt6u4iI3AbcBjB27NBskXg8ysu7q1hQkG2jGYNgjCuJG+aM5pnNpXxtwURGpoVvueO98gaKSuv5wVVTbHGRMLR0Zu6gDuLzp3QTA8wCHlHVmUAzcG8fx14NvKmqdQAiMhy4Fu8/FjlAsojc3NuJqvqYqs5R1TlZWVkDfBuRoaisnpoTbdatMoi+vmCir1a/3+lQzsjyjSUkxEbxqdmjnQ7FhCB/En0ZUKaqm3w/P4s38ffmRj5atrkEOKSqNaraAawEzj/dYCPd2uIqoqOEhQXW/zlYxriS+PTs0TzzTimVDa1Oh3NaGls7WFVUwTXTc0hLjHU6HBOC+k30qloJlIpI15OBxUBxz+N8tfyLgVXdNh8B5otIkng/Ty6mx4Nc86E1uyqZn+8iLcn+WIPpjoUT8XiURzeEZ61+5dYyWjrcNhLW9MnfXjd3AStEZAcwA3hARG4Xkdu7HXMd3hr8B7V736eAZ4F3gZ2+6z0WiMAjzYGaJg7UNHOpTUkcdF2t+j9sPhJ2rXpVZfmmI5w7Oo1zbdyF6YNfiV5Vi3z183NVdamqHlfVR1X10W7HPKWqH+s6qao/VNVJqjpVVb+gqm2BfAORomsSs0usPu+IcG3VbzpUx/7qJm4utNa86ZuNjA0Ra4urOCdnGKOH22yDThjjSuJTs7yt+qrG8GnVL99YwrCEGK6enuN0KCaEWaIPATUn2nj3yHHrbeOwrlZ9uIyWrTnRxupdlXx69hgS46w7rumbJfoQsG53FapYonfY2IzwatX/eUspHW61kbCmX5boQ8Da4ipy0xOZMmqY06EMeXcsnIg7DGr1bo/yh01HOH9CBhOyUpwOx4Q4S/QOa27r5PX9tVw6ZYSNaAwB3lZ9Ln/YdITqEG7Vr99bTXl9i3WpNH6xRO+w19+vob3TY5OYhZA7F55Fp0d5JIRb9cs3lpCdGm/lPuMXS/QOW1NcRVpiLPPGu5wOxfiEequ+tO4k6/fVcOPcMcRG25+w6Z/9ljio0+3hlT3VLJqUTYz9wYaUrlb9oxsOOh3Kx/xh8xEEuHGePYQ1/rHs4qB3Dh+n/mSHLRkYgsZmJHH9zFxWbCoJqVZ9W6ebP79TyuLJIwK6MIWJbJboHbS2uIq4mCguOntoztYZ6u5cNDHkWvUvvVfJseZ2ewhrBsQSvUNUlTXFlXxiQgbJ8f4sC2CCbVxG8oet+hOh0apfsfEI4zKSuHBiptOhmDBiid4heypPUHa8hcvOGel0KOYUulr1vw6BVv3eyhNsPlzH5+eNtfWEzYBYonfI2uIqRGDxZJt7PpSNy0jmupm5LN/ofKt+xaYS4mKiuGHOGEfjMOHHEr1D1hRXMmNMOtmp4bt83VBx50LnW/XNbZ2sfLecK6eNwpUc51gcJjxZondARX0L75U3ctkUK9uEg/GZySyd4WytflVRBU1tndxs89qY02CJ3gEv7/bOPW+jGsPHXYsm0uFWHnOgVa+qLN9YwqSRqcwaOzzo1zfhzxK9A9bsqiI/K5mJ2TYZVbjoatUv31RCzYngrp2zrbSe4qON3Dx/nM2HZE6LJfoga2jpYOPBY9aaD0MftOpfC+4cOMs3lpAcF83SmblBva6JHJbog2z93mo6PWqjYcPQ+Mxkrp2Rw+83Bq9Vf7y5nRd3HOW6Wbmk2HgLc5os0QfZmuIqMlPimTHGaq3h6K5FZ9He6Qlaq/4vW0tp7/TYSFhzRiIm0T+3rZxPPPgKeff+nU88+ArPbSt3OqSPaet0s2FvDZdMzibaBryEpbzMZJbOzA1Kq97jUVZsOsKcccOZNNIWpTGnLyIS/XPbyrlv5U7K61tQoLy+hftW7gy5ZP/2gWM0tXVafT7MdbXqf/P64PbAeWN/LSXHTlpr3pyxiEj0y1bvpaXD/ZFtLR1ulq3e61BEvVtbXEVSXDSfsHlKwlqerwfO028fprZp8Fr1yzeW4EqO4/JpNt7CnJmISPQV9S0D2u4Ej0d5eXcVF52VRUJstNPhmDN056KJvlr94LTqjza08PLuKm6YM5r4GPt9MWfGr0QvIuki8qyI7BGR3SJyXo/994hIke/rPRFxi4jLn3MDoa95ueNjozjaEBrJfkd5A1WNbVa2iRD5WSksnZHL798uGZRW/TObS1HgpnlWtjFnzt8W/a+Al1R1EjAd2N19p6ouU9UZqjoDuA/YoKp1/pwbCPcsKSCxRys5Jkpwe5RLHtrAb18/SKfbE+jLDsja4kqio4RFk2wSs0hx56KJtHW6+U2AW/Udbg9/3HyEi8/OYmxGUkBf2wxN/SZ6EUkDLgIeB1DVdlWtP8UpnwOeOc1zT8vSmbn89Ppp5KYnIkBueiI/v2E6r9y9gHl5Ln7y991c/fCbvHvkeKAv7bc1u6qYO344w21CqoiRn5XCtTNyeTrArfqXi6uoPtHGzYXWmjeB4U+LPg+oAZ4UkW0i8lsRSe7tQBFJAj4J/PU0zr1NRLaIyJaampoBv5GlM3N5895FHHrwSt68dxFLZ+YyxpXEE7fM5dGbZ1F/sp1PPfIW963cSf3J9gG//pk4XNvM+9VNNolZBBqMVv3yTSXkpiey0D79mQDxJ9HHALOAR1R1JtAM3NvHsVcDb3Yr2/h9rqo+pqpzVHVOVlbgltYTET45dRRrv30xX/lEHn/eUsrihzaw8t0yVDVg1zmVtcU2iVmkmpCVwjXTc3j67RKOBaBVf7CmiTf3H+Nz88bYWAsTMP4k+jKgTFU3+X5+Fm/y7s2N+Mo2p3HuoEqJj+H7V03hhTsvYGxGEt/+83Y+95uN7K9uGvRrrymuZNLIVMa4rN4aie5cdBZtnW4eC0C/+hWbjhATJXxmri0uYgKn30SvqpVAqYgU+DYtBop7Huerx18MrBroucE0JWcYf739fB64bhq7j57g8l+9xs9X76W1Rz/8QDnW1MbWkuO2ZGAEm5jta9W/dWat+tYON89uLWPJ1JG2II0JKH973dwFrBCRHcAM4AERuV1Ebu92zHXAGlVt7u/cMwv5zEVFCZ8vHMu6uy/m6uk5PPzqfi79xQZe3Vsd8Gut21ONR7FJzCJcV6v+N68fOu3XeGF7BQ0tHfYQ1gScBKtOPRBz5szRLVu2BO16bx2o5T+ee48DNc1cMW0kP7jqHEamBaZF9b9+t4XiigbevHeRzSUe4b75x22sLa7i9X9bSEZK/IDPv/Z/3qS5rZO137rIflfMgInIVlWd09u+iBgZe6bOn5DJP795EfcsKWDd7moWP7Sex984dMZ971va3byxv4ZLp4ywP9wh4K5FZ9HScXqt+vfKG9heWs9NhWPtd8UEnCV6n7iYKO5YOJG137qYuXku7n+xmGsefpNtZ9D3/vX3a2jt8HCpdascEj6o1b99mLrmgXXhXb6xhMTYaK6fNXqQojNDmSX6HsZmJPHkLXN55KZZHGtu4/pH3uJ7f9tJw8mOAb/WmuIqUhNiKMx3DUKkJhTdtWiir1Xvfw+cxtYOVhVVcM30HNISYwcxOjNUWaLvhYhw+bRRrLt7Abeen8czm4+w+L/X87dt/ve973R7WLe7ikWTsomNtts8VEzMTuXqc3P43Vv+t+pXbi2jpcNt0xGbQWMZ6BRS4mP4wdVTeP7OC8gdnsS3/rSdz/9mEwdq+u97v7XkOMdPdtggqSHoG4v9b9WrKss3HWH66DSmjU4LQnRmKLJE74epuWms/Nr5/GTpVHZVNHD5L1/noTWn7nu/triKuOgoLj47cKN8TXjoatU/7UerftOhOvZXN3GTtebNILJE76foKOHm+eNYd/cCrjx3FP/3lf1c9ovXWN9L33tVZe3uKs6bkEFqgtVch6JvLJ7IyQ43v+2nVb98YwnDEmK4+tycIEVmhiJL9AOUlRrPLz47gz/8r0JiooVbnnyHO1a8S1Vj6wfH7KtqouTYSSvbDGETs1O5qp9affWJVlbvquTTs8eQGGeLi5jBY4n+NJ0/MZN/fvNC7r70bNburmLxQxt48s1D/HVrGTc8+hYAD7+yP+TWrTXB841Fp27V//mdUjrcyk3zxwY5MjPUxDgdQDiLj4nmrsVncc2MHP5j1S5+9EIxAnT1y6lsbOW+lTsB7zTKZmg5a8SHrfqvXpj/kbUI3B7lmc2lnD8hgwlZKQ5GaYYCa9EHwLiMZH5361xcSXH07HwZiouUm+D5oFX/xkdb9a/uqaa8vsW6VJqgsEQfICLC8T4WNAmlRcpNcJ01IpUrp43iqTcPc7xbrX75phKyU+PtOY4JCkv0AdTXIuV9bTdDwzcWn8XJDjePv+GdA6e07iQb9tVw49wxNpjOBIX9lgVQb4uUJ8ZGc8+Sgj7OMEPB2SNSuWLaKJ56y9uqX7HpCALcOM8ewprgsEQfQL0tUv7T66fZg1jDNxadRXNbJxf+7FUe3XCAuOgoNh+q6/9EYwLAet0E2NKZuZbYzcfsPtpIlAhNbZ0AtHZ6rEeWCRpr0RsTBMtW78XdY0I865FlgsUSvTFB0FfPK+uRZYLBEr0xQWA9soyTLNEbEwTWI8s4yR7GGhMEXQ9cl63eS0V9CznpidyzpMAexJqgsERvTJBYjyzjFCvdGGNMhLNEb4wxEc4SvTHGRDhL9MYYE+Es0RtjTIQT1Z5LZThPRGqAktM8PROoDWA44czuxUfZ/fgoux8fioR7MU5Vs3rbEZKJ/kyIyBZVneN0HKHA7sVH2f34KLsfH4r0e2GlG2OMiXCW6I0xJsJFYqJ/zOkAQojdi4+y+/FRdj8+FNH3IuJq9MYYYz4qElv0xhhjurFEb4wxES6sE72IPCEi1SLyXrdtLhFZKyLv+/473MkYg6WPe7FMRPaIyA4R+ZuIpDsYYlD1dj+67btbRFREMp2ILdj6uhcicpfv92OXiPzMqfiCrY+/lRkislFEikRki4jMczLGQAvrRA88BXyyx7Z7gXWqehawzvfzUPAUH78Xa4GpqnousA+4L9hBOegpPn4/EJExwGXAkWAH5KCn6HEvRGQhcC0wXVXPAX7uQFxOeYqP/278DPiRqs4AfuD7OWKEdaJX1deAuh6brwV+5/v+d8DSYMbklN7uhaquUdVO348bgdFBD8whffxuAPwC+DdgyPRC6ONefA14UFXbfMdUBz0wh/RxPxQY5vs+DagIalCDLKwTfR9GqOpR3/eVwAgngwkhXwb+6XQQThKRa4FyVd3udCwh4GzgQhHZJCIbRGSu0wE57F+BZSJSivfTTUR9+o3ERP8B9fYdHTItt76IyPeATmCF07E4RUSSgH/H+7HceFeXcwHzgXuAP4uIOBuSo74GfEtVxwDfAh53OJ6AisREXyUiowB8/x0yH0l7IyK3AFcBN+nQHjQxAcgDtovIYbxlrHdFZKSjUTmnDFipXpsBD96JvYaqLwErfd//BbCHsSHuebz/0/D9d5WDsThKRD6Jtx59jaqedDoeJ6nqTlXNVtXxqjoeb6KbpaqVDofmlOeAhQAicjYQR/jP3ngmKoCLfd8vAt53MJaAC+tELyLPAG8DBSJSJiJfAR4ELhWR94FLfD9HvD7uxcNAKrDW123sUUeDDKI+7seQ1Me9eALI93Ux/CPwpaHyia+P+/FV4CER2Q48ANzmZIyBZlMgGGNMhAvrFr0xxpj+WaI3xpgIZ4neGGMinCV6Y4yJcJbojTEmwlmiN8aYCGeJ3hhjItz/B4Rj/qrQHTvMAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "alpha_list=[i for i in range(10,20)]\n",
    "loss_RF_tot=[]\n",
    "acc_RF_tot=[]\n",
    "for alpha in alpha_list:    \n",
    "    print(alpha)\n",
    "    model_RF=RandomForestRegressor(max_depth=alpha)\n",
    "    loss_RF,acc_RF=model_result(model_RF,X_train_vec,y_train_vec,X_test_vec,y_test_vec)\n",
    "    loss_RF_tot.append(loss_RF)\n",
    "    acc_RF_tot.append(acc_RF)\n",
    "    \n",
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "ax.scatter(alpha_list,loss_RF_tot)\n",
    "ax.plot(alpha_list,loss_RF_tot) \n",
    "plt.title(\"Loss for RF model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.75432004595493 0.13152715390193143\n",
      "6.665527210403895 0.13174374476179715\n"
     ]
    }
   ],
   "source": [
    "model_RF=RandomForestRegressor(max_depth=np.argmin(np.array(loss_RF_tot))+10)\n",
    "loss_RF,acc_RF=model_result(model_RF,X_train_vec,y_train_vec,X_test_vec,y_test_vec)\n",
    "print(loss_RF,acc_RF)\n",
    "loss_RF_num,acc_RF_num=model_result(model_RF,X_train_num_vec,y_train_vec,X_test_num_vec,y_test_vec)\n",
    "print(loss_RF_num,acc_RF_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>favorites_count</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>statuses_count</th>\n",
       "      <th>friends_count</th>\n",
       "      <th>verified</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>TweetID</th>\n",
       "      <th>urls_count</th>\n",
       "      <th>hashtags_count</th>\n",
       "      <th>month</th>\n",
       "      <th>wday</th>\n",
       "      <th>hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[descendue, pourcentage, vote]</td>\n",
       "      <td>0</td>\n",
       "      <td>85</td>\n",
       "      <td>4442</td>\n",
       "      <td>327</td>\n",
       "      <td>0</td>\n",
       "      <td>['presidentielle2022', 'hidalgo']</td>\n",
       "      <td>1647607994000</td>\n",
       "      <td>1184643</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[photo, demande, arbre, généalogique]</td>\n",
       "      <td>0</td>\n",
       "      <td>427</td>\n",
       "      <td>33282</td>\n",
       "      <td>443</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>1647601275000</td>\n",
       "      <td>1199190</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[dossiers, promis, poste, gars, vraiment, fiable]</td>\n",
       "      <td>6</td>\n",
       "      <td>1127</td>\n",
       "      <td>13111</td>\n",
       "      <td>1596</td>\n",
       "      <td>0</td>\n",
       "      <td>['macron']</td>\n",
       "      <td>1646900684000</td>\n",
       "      <td>917372</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[lol, cas, faut, second, tour, macron, sortir,...</td>\n",
       "      <td>2</td>\n",
       "      <td>1699</td>\n",
       "      <td>25760</td>\n",
       "      <td>2036</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>1647161294000</td>\n",
       "      <td>731754</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[zemmour, putain, discours, propagandiste, lit...</td>\n",
       "      <td>0</td>\n",
       "      <td>249</td>\n",
       "      <td>20718</td>\n",
       "      <td>369</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>1647724874000</td>\n",
       "      <td>1400049</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  favorites_count  \\\n",
       "0                     [descendue, pourcentage, vote]                0   \n",
       "1              [photo, demande, arbre, généalogique]                0   \n",
       "2  [dossiers, promis, poste, gars, vraiment, fiable]                6   \n",
       "3  [lol, cas, faut, second, tour, macron, sortir,...                2   \n",
       "4  [zemmour, putain, discours, propagandiste, lit...                0   \n",
       "\n",
       "   followers_count  statuses_count  friends_count  verified  \\\n",
       "0               85            4442            327         0   \n",
       "1              427           33282            443         0   \n",
       "2             1127           13111           1596         0   \n",
       "3             1699           25760           2036         0   \n",
       "4              249           20718            369         0   \n",
       "\n",
       "                            hashtags      timestamp  TweetID  urls_count  \\\n",
       "0  ['presidentielle2022', 'hidalgo']  1647607994000  1184643           0   \n",
       "1                                 []  1647601275000  1199190           0   \n",
       "2                         ['macron']  1646900684000   917372           0   \n",
       "3                                 []  1647161294000   731754           1   \n",
       "4                                 []  1647724874000  1400049           2   \n",
       "\n",
       "   hashtags_count  month  wday  hour  \n",
       "0               2      3     4    12  \n",
       "1               0      3     4    11  \n",
       "2               1      3     3     8  \n",
       "3               0      3     6     8  \n",
       "4               0      3     5    21  "
      ]
     },
     "execution_count": 552,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "eval_data = pd.read_csv(\"data/evaluation.csv\")\n",
    "\n",
    "# Pre-process the traning data\n",
    "label = \"text\"\n",
    "eval_data[label] = eval_data[label].map(lambda x: str.split(x, sep=\" \"))\n",
    "\n",
    "\n",
    "label = \"urls\"\n",
    "eval_data[label] = eval_data[label].map(lambda x: [] if x==\"[]\" else [str.strip(url) for url in str.split(x[1:-1], sep=\",\")])\n",
    "eval_data[label+\"_count\"] = eval_data[label].map(lambda x: len(x))\n",
    "\n",
    "\n",
    "label = \"hashtags\"\n",
    "eval_data[label] = eval_data[label].map(lambda x: [] if x==\"[]\" else [str.strip(tag) for tag in str.split(x[1:-1], sep=\",\")])\n",
    "eval_data[label+\"_count\"] = eval_data[label].map(lambda x: len(x))\n",
    "\n",
    "\n",
    "\n",
    "# Treatment of time\n",
    "# The time relative to the election (month and yday), day in the week, \n",
    "# and the hour all affect the number of retweets.\n",
    "eval_data[\"time\"] = eval_data[\"timestamp\"].map(lambda x: time.gmtime(x//1000))\n",
    "# Only 2380 tweets are not posted in 2022, so we ignore the year\n",
    "# eval_data[\"year\"] = eval_data[\"timestamp\"].map(lambda x: x.tm_year)\n",
    "eval_data[\"month\"] = eval_data[\"time\"].map(lambda x: x.tm_mon)\n",
    "eval_data[\"wday\"] = eval_data[\"time\"].map(lambda x: x.tm_wday)\n",
    "eval_data[\"hour\"] = eval_data[\"time\"].map(lambda x: x.tm_hour)\n",
    "\n",
    "\n",
    "# We drop the following data:\n",
    "# @TweetID: useless\n",
    "# @mentions: none of the mentions in the eval data are not null\n",
    "# @mentions_count\n",
    "# @urls: we use urls_count instead\n",
    "\n",
    "eval_data.drop(labels=[\"time\", \"mentions\", \"urls\"], axis=1, inplace=True)\n",
    "eval_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(117990, 11)"
      ]
     },
     "execution_count": 553,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_eval = eval_data.drop(labels=[\"TweetID\"], axis=1)\n",
    "    \n",
    "standardise(X_eval,[\"favorites_count\",\"followers_count\",\"statuses_count\",\"friends_count\",\"urls_count\",\"hashtags_count\",\"verified\",\"timestamp\",\"month\",\"wday\",\"hour\"])\n",
    "\n",
    "# split the table into 2 parts: one with the text and the other with the numbers\n",
    "X_eval_num=X_eval.drop([\"text\", \"hashtags\"], axis=1)\n",
    "\n",
    "\n",
    "X_eval_text=X_eval[[\"text\", \"hashtags\"]]\n",
    "\n",
    "X_eval_num_vec=X_eval_num.values\n",
    "X_eval_num_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text to vector\n",
    "eval_text_tensor=torch.Tensor(text2vec(X_eval_text[\"text\"], model=model_text128))\n",
    "\n",
    "# hashtags to vector\n",
    "eval_tags_tensor=torch.Tensor(text2vec(X_eval_text[\"hashtags\"], model=model_text128))\n",
    "\n",
    "# combine the two vectors\n",
    "X_eval_text_tensor = torch.cat((eval_text_tensor, eval_tags_tensor), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([117990, 11])\n",
      "torch.Size([117990, 256])\n",
      "torch.Size([117990, 267])\n"
     ]
    }
   ],
   "source": [
    "X_eval_num_vec=X_eval_num.to_numpy()\n",
    "X_eval_text_vec=X_eval_text_tensor.numpy()\n",
    "X_eval_vec=np.hstack([X_eval_num_vec,X_eval_text_vec])\n",
    "\n",
    "X_eval_num_tensor=torch.Tensor(X_eval_num.values)\n",
    "\n",
    "X_eval_tensor=torch.cat((X_eval_num_tensor, X_eval_text_tensor), axis=1)\n",
    "\n",
    "print(X_eval_num_tensor.shape)\n",
    "print(X_eval_text_tensor.shape)\n",
    "print(X_eval_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 822,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\11209\\OneDrive\\3A\\INF554\\inf554\\project_vec.ipynb Cell 22\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/11209/OneDrive/3A/INF554/inf554/project_vec.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model_RF\u001b[39m=\u001b[39mRandomForestRegressor(max_depth\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39margmin(np\u001b[39m.\u001b[39marray(loss_RF_tot))\u001b[39m+\u001b[39m\u001b[39m10\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/11209/OneDrive/3A/INF554/inf554/project_vec.ipynb#X26sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model_RF\u001b[39m.\u001b[39;49mfit(np\u001b[39m.\u001b[39;49mconcatenate((X_train_num_vec,X_test_num_vec)), np\u001b[39m.\u001b[39;49mconcatenate((y_train_vec, y_test_vec)))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/11209/OneDrive/3A/INF554/inf554/project_vec.ipynb#X26sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m pred_RF \u001b[39m=\u001b[39m model_RF\u001b[39m.\u001b[39mpredict(X_eval_num_vec)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/11209/OneDrive/3A/INF554/inf554/project_vec.ipynb#X26sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrite_result\u001b[39m(filename,results):\n",
      "File \u001b[1;32mc:\\Users\\11209\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:442\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    431\u001b[0m trees \u001b[39m=\u001b[39m [\n\u001b[0;32m    432\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_estimator(append\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[0;32m    433\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    434\u001b[0m ]\n\u001b[0;32m    436\u001b[0m \u001b[39m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    437\u001b[0m \u001b[39m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    438\u001b[0m \u001b[39m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    439\u001b[0m \u001b[39m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    440\u001b[0m \u001b[39m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    441\u001b[0m \u001b[39m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 442\u001b[0m trees \u001b[39m=\u001b[39m Parallel(\n\u001b[0;32m    443\u001b[0m     n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs,\n\u001b[0;32m    444\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    445\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m_joblib_parallel_args(prefer\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mthreads\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m    446\u001b[0m )(\n\u001b[0;32m    447\u001b[0m     delayed(_parallel_build_trees)(\n\u001b[0;32m    448\u001b[0m         t,\n\u001b[0;32m    449\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[0;32m    450\u001b[0m         X,\n\u001b[0;32m    451\u001b[0m         y,\n\u001b[0;32m    452\u001b[0m         sample_weight,\n\u001b[0;32m    453\u001b[0m         i,\n\u001b[0;32m    454\u001b[0m         \u001b[39mlen\u001b[39;49m(trees),\n\u001b[0;32m    455\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    456\u001b[0m         class_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclass_weight,\n\u001b[0;32m    457\u001b[0m         n_samples_bootstrap\u001b[39m=\u001b[39;49mn_samples_bootstrap,\n\u001b[0;32m    458\u001b[0m     )\n\u001b[0;32m    459\u001b[0m     \u001b[39mfor\u001b[39;49;00m i, t \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(trees)\n\u001b[0;32m    460\u001b[0m )\n\u001b[0;32m    462\u001b[0m \u001b[39m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    463\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_\u001b[39m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\joblib\\parallel.py:1046\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1043\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1044\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1046\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1047\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1050\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1051\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1052\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\joblib\\parallel.py:861\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    859\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    860\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 861\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[0;32m    862\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\joblib\\parallel.py:779\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    777\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    778\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> 779\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[0;32m    780\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    781\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    782\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    783\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    784\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m     \u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\joblib\\_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m    570\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    571\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 572\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\joblib\\parallel.py:262\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    259\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 262\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    263\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\joblib\\parallel.py:262\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    259\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 262\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    263\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\11209\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\fixes.py:211\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    210\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig):\n\u001b[1;32m--> 211\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\11209\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:185\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    182\u001b[0m     \u001b[39melif\u001b[39;00m class_weight \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbalanced_subsample\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    183\u001b[0m         curr_sample_weight \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m compute_sample_weight(\u001b[39m\"\u001b[39m\u001b[39mbalanced\u001b[39m\u001b[39m\"\u001b[39m, y, indices\u001b[39m=\u001b[39mindices)\n\u001b[1;32m--> 185\u001b[0m     tree\u001b[39m.\u001b[39;49mfit(X, y, sample_weight\u001b[39m=\u001b[39;49mcurr_sample_weight, check_input\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    186\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     tree\u001b[39m.\u001b[39mfit(X, y, sample_weight\u001b[39m=\u001b[39msample_weight, check_input\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\11209\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\tree\\_classes.py:1315\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m   1278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\n\u001b[0;32m   1279\u001b[0m     \u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, check_input\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, X_idx_sorted\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdeprecated\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1280\u001b[0m ):\n\u001b[0;32m   1281\u001b[0m     \u001b[39m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[0;32m   1282\u001b[0m \n\u001b[0;32m   1283\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1312\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1313\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1315\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m   1316\u001b[0m         X,\n\u001b[0;32m   1317\u001b[0m         y,\n\u001b[0;32m   1318\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m   1319\u001b[0m         check_input\u001b[39m=\u001b[39;49mcheck_input,\n\u001b[0;32m   1320\u001b[0m         X_idx_sorted\u001b[39m=\u001b[39;49mX_idx_sorted,\n\u001b[0;32m   1321\u001b[0m     )\n\u001b[0;32m   1322\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\11209\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\tree\\_classes.py:420\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    409\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    410\u001b[0m     builder \u001b[39m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    411\u001b[0m         splitter,\n\u001b[0;32m    412\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    417\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    418\u001b[0m     )\n\u001b[1;32m--> 420\u001b[0m builder\u001b[39m.\u001b[39;49mbuild(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtree_, X, y, sample_weight)\n\u001b[0;32m    422\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m is_classifier(\u001b[39mself\u001b[39m):\n\u001b[0;32m    423\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_RF=RandomForestRegressor(max_depth=np.argmin(np.array(loss_RF_tot))+10)\n",
    "model_RF.fit(np.concatenate((X_train_num_vec,X_test_num_vec)), np.concatenate((y_train_vec, y_test_vec)))\n",
    "pred_RF = model_RF.predict(X_eval_num_vec)\n",
    "\n",
    "def write_result(filename,results):\n",
    "    with open(filename, 'w',newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"TweetID\", \"retweets_count\"])\n",
    "        for index, prediction in enumerate(results):\n",
    "            writer.writerow([str(eval_data['TweetID'].iloc[index]) , str(int(np.round(prediction)))])\n",
    "            \n",
    "print(pred_RF.shape)      \n",
    "write_result(\"data/RF_predictions.txt\", pred_RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_size, n_feature=6, output_size=10):\n",
    "        super(CNN, self).__init__()\n",
    "        #insert your code here\n",
    "        K=5 #Kernel size\n",
    "        self.n_feature = n_feature\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=n_feature, kernel_size=K)\n",
    "        # self.conv2 = nn.Conv1d(n_feature, n_feature, kernel_size=K)\n",
    "        self.fc1 = nn.Linear(n_feature*31, output_size)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        # self.batchnorm = nn.BatchNorm1d(20, momentum=0.1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, verbose=False):\n",
    "        #initial dimensions for x will be [24, 256]\n",
    "        #insert your code here\n",
    "        x=x.unsqueeze(dim=1) # [24, 1, 128]\n",
    "        b = len(x)\n",
    "        x = self.conv1(x) #[24, 6, 124]\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool1d(x, kernel_size=4) #[24, 6, 31]\n",
    "        # x = self.conv2(x)  # [24, 6, 27]\n",
    "        # x = F.relu(x)\n",
    "        # x = F.max_pool1d(x, kernel_size=4) #[24, 6, 6]\n",
    "        # print(\"x.shape\",x.shape) \n",
    "        x = x.reshape(-1, self.n_feature*31) \n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    \n",
    "def initilize_model_CNN(input_size, \n",
    "                    n_feature=5,\n",
    "                    output_size=10,\n",
    "                    learning_rate=0.01):\n",
    "\n",
    "    cnn_model = CNN(input_size=input_size,\n",
    "                    n_feature=n_feature,\n",
    "                    output_size=output_size)\n",
    "    \n",
    "    cnn_model.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(cnn_model.parameters(),\n",
    "                           lr=learning_rate)\n",
    "    \n",
    "    # L1loss is the MAE loss\n",
    "    loss_function = nn.L1Loss()\n",
    "    \n",
    "    return cnn_model, optimizer, loss_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_loader, epoch=100):\n",
    "    log_interval= max(epoch//10, 1)\n",
    "    #set model in train mode\n",
    "    model.train() \n",
    "    t0_epoch = time.time()\n",
    "    for epoch_i in range(epoch):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            # print(output.shape)\n",
    "            # print(\"data\",data.shape)\n",
    "            # print(output.shape, target.shape)\n",
    "            loss = F.l1_loss(output, target)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        if epoch_i % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f} \\ttime_elapsed: {:^9.2f}'.format(\n",
    "                epoch, epoch_i+1, epoch,\n",
    "                100. * (epoch_i+1) / epoch, loss.item(), time.time()-t0_epoch))\n",
    "                \n",
    "\n",
    "def evaluate(model, loss_function, eval_loader):\n",
    "\n",
    "    #set model in eval mode\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "    # For each batch in our validation set...\n",
    "    for step, (data, target) in enumerate(eval_loader):\n",
    "        # print(data.shape, target.shape)\n",
    "        # break\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_function(output, target)\n",
    "        val_loss.append(loss)\n",
    "\n",
    "        # Calculate the accuracy rate\n",
    "        accuracy = (output == target).cpu().numpy().mean() * 100\n",
    "        val_accuracy.append(accuracy)\n",
    "\n",
    "    # Compute the average accuracy and loss over the validation set.\n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_accuracy = np.mean(val_accuracy)\n",
    "\n",
    "    return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 24\n",
    "dataset_train_text = TensorDataset(X_train_text_tensor.to(device), y_train_tensor.to(device))\n",
    "X_train_loader_text = DataLoader(dataset=dataset_train_text, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "dataset_train_tweet = TensorDataset(train_text_tensor.to(device), y_train_tensor.to(device))\n",
    "X_train_loader_tweet = DataLoader(dataset=dataset_train_tweet , batch_size=batch_size, shuffle=True)\n",
    "\n",
    "dataset_train_tags = TensorDataset(train_tags_tensor.to(device), y_train_tensor.to(device))\n",
    "X_train_loader_tags = DataLoader(dataset=dataset_train_tags, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "247778\n"
     ]
    }
   ],
   "source": [
    "input_size = X_train_text_vec.shape[1]\n",
    "cnn_model, optimizer, loss_function = initilize_model_CNN(input_size=input_size,\n",
    "                                                      n_feature=6,\n",
    "                                                      output_size=1,\n",
    "                                                      learning_rate=1e-5)\n",
    "print(len(dataset_train_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train(cnn_model, optimizer, train_loader=X_train_loader_text, epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test_text = TensorDataset(X_test_text_tensor[:400].to(device), y_test_tensor[:400].to(device))\n",
    "X_test_loader_text = DataLoader(dataset=dataset_test_text, shuffle=True)\n",
    "\n",
    "dataset_test_tweet = TensorDataset(test_text_tensor.to(device), y_test_tensor.to(device))\n",
    "X_test_loader_tweet = DataLoader(dataset=dataset_test_tweet, shuffle=True)\n",
    "\n",
    "dataset_test_tags = TensorDataset(test_tags_tensor.to(device), y_test_tensor.to(device))\n",
    "X_test_loader_tags = DataLoader(dataset=dataset_test_tags, shuffle=True)\n",
    "\n",
    "# evaluate(cnn_model, loss_function, eval_loader=X_test_loader_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we define the full internet\n",
    "class FNN(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_num_size,\n",
    "                 input_tweet_size,  \n",
    "                 input_tags_size, \n",
    "                 dense_layers, \n",
    "                 output_tweet_size, \n",
    "                 output_tags_size, \n",
    "                 dropout):\n",
    "        super(FNN, self).__init__()\n",
    "        \n",
    "        self.input_num_size = input_num_size\n",
    "        self.input_tweet_size = input_tweet_size\n",
    "        self.input_tags_size = input_tags_size\n",
    "        self.tot_size = input_num_size + output_tweet_size + output_tags_size\n",
    "        \n",
    "        self.CNN_tweet = CNN(input_size=input_tweet_size, output_size=output_tweet_size)\n",
    "        self.CNN_tags = CNN(input_size=input_tweet_size, output_size=output_tags_size)\n",
    "        \n",
    "        self.dense1 = nn.Linear(self.tot_size, dense_layers[0])\n",
    "        self.dense2 = nn.Linear(dense_layers[0], dense_layers[1])\n",
    "        self.dense3 = nn.Linear(dense_layers[1], 1)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.batchnorm= nn.BatchNorm1d(dense_layers[0])\n",
    "\n",
    "    def forward(self, x, verbose=False):\n",
    "        \n",
    "        # assert(self.tot_size==len(x))\n",
    "        # print(x.shape)\n",
    "        x_num = x[:,:self.input_num_size]\n",
    "        x_tweet = x[:,self.input_num_size: self.input_num_size + self.input_tweet_size]\n",
    "        x_tags = x[:,self.input_num_size + self.input_tweet_size:]\n",
    "        \n",
    "        x_tweet = self.CNN_tweet(x_tweet)\n",
    "\n",
    "        x_tags= self.CNN_tags(x_tags)\n",
    " \n",
    "        x_dense = torch.cat((x_num, x_tweet, x_tags), axis=1)\n",
    "        \n",
    "        x_dense = self.dense1(x_dense)\n",
    "        x_dense = F.relu(x_dense)\n",
    "        \n",
    "        x_dense = self.dropout(x_dense)\n",
    "        x_dense = self.batchnorm(x_dense)\n",
    "        \n",
    "        x_dense = self.dense2(x_dense)\n",
    "        \n",
    "        res = self.dense3(F.relu(x_dense))\n",
    "        return F.relu(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initilize_model_FNN(input_num_size,\n",
    "                        input_tweet_size,  \n",
    "                        input_tags_size, \n",
    "                        dense_layers=[50, 10], \n",
    "                        output_tweet_size=10, \n",
    "                        output_tags_size=5, \n",
    "                        dropout=0.5,\n",
    "                        learning_rate=1e-3):\n",
    "\n",
    "    fnn_model = FNN(input_num_size=input_num_size,\n",
    "                    input_tweet_size=input_tweet_size,  \n",
    "                    input_tags_size=input_tags_size, \n",
    "                    dense_layers=dense_layers, \n",
    "                    output_tweet_size=output_tweet_size, \n",
    "                    output_tags_size=output_tags_size, \n",
    "                    dropout=dropout)\n",
    "    \n",
    "    fnn_model.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(cnn_model.parameters(),\n",
    "                           lr=learning_rate)\n",
    "    \n",
    "    # L1loss is the MAE loss\n",
    "    loss_function = nn.L1Loss()\n",
    "    \n",
    "    return fnn_model, optimizer, loss_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size=24\n",
    "dataset_train = TensorDataset(X_train_tensor.to(device), y_train_tensor.to(device))\n",
    "X_train_loader = DataLoader(dataset=dataset_train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "input_num_size = X_train_num_vec.shape[1]\n",
    "input_tweet_size = X_train_tweet_vec.shape[1]\n",
    "input_tags_size = X_train_tags_vec.shape[1]\n",
    "fnn_model, optimizer, loss_function = initilize_model_FNN(input_num_size, \n",
    "                                                          input_tweet_size, \n",
    "                                                          input_tags_size, \n",
    "                                                          output_tweet_size=10, \n",
    "                                                          output_tags_size=5, \n",
    "                                                          dropout=0.7,\n",
    "                                                          learning_rate=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "247800\n",
      "Train Epoch: 10 [1/10 (10%)]\tLoss: 1.560114 \ttime_elapsed:   20.54  \n",
      "Train Epoch: 10 [2/10 (20%)]\tLoss: 0.000000 \ttime_elapsed:   41.37  \n",
      "Train Epoch: 10 [3/10 (30%)]\tLoss: 0.927063 \ttime_elapsed:   62.35  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\11209\\OneDrive\\3A\\INF554\\inf554\\project_vec.ipynb Cell 32\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/11209/OneDrive/3A/INF554/inf554/project_vec.ipynb#X60sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(input_tags_size)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/11209/OneDrive/3A/INF554/inf554/project_vec.ipynb#X60sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(X_train_loader)\u001b[39m*\u001b[39mbatch_size)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/11209/OneDrive/3A/INF554/inf554/project_vec.ipynb#X60sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m train(fnn_model, optimizer, train_loader\u001b[39m=\u001b[39;49mX_train_loader, epoch\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\11209\\OneDrive\\3A\\INF554\\inf554\\project_vec.ipynb Cell 32\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, train_loader, epoch)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/11209/OneDrive/3A/INF554/inf554/project_vec.ipynb#X60sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m t0_epoch \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/11209/OneDrive/3A/INF554/inf554/project_vec.ipynb#X60sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch_i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epoch):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/11209/OneDrive/3A/INF554/inf554/project_vec.ipynb#X60sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mfor\u001b[39;00m batch_idx, (data, target) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/11209/OneDrive/3A/INF554/inf554/project_vec.ipynb#X60sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         data, target \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mto(device), target\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/11209/OneDrive/3A/INF554/inf554/project_vec.ipynb#X60sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\11209\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:517\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    516\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[1;32m--> 517\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    518\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    519\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    520\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    521\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\11209\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:557\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    555\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    556\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 557\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    558\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    559\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data)\n",
      "File \u001b[1;32mc:\\Users\\11209\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:47\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     46\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 47\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[1;32mc:\\Users\\11209\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:83\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39meach element in list of batch should be of equal size\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     82\u001b[0m     transposed \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch)\n\u001b[1;32m---> 83\u001b[0m     \u001b[39mreturn\u001b[39;00m [default_collate(samples) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]\n\u001b[0;32m     85\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[39m.\u001b[39mformat(elem_type))\n",
      "File \u001b[1;32mc:\\Users\\11209\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:83\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39meach element in list of batch should be of equal size\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     82\u001b[0m     transposed \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch)\n\u001b[1;32m---> 83\u001b[0m     \u001b[39mreturn\u001b[39;00m [default_collate(samples) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]\n\u001b[0;32m     85\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[39m.\u001b[39mformat(elem_type))\n",
      "File \u001b[1;32mc:\\Users\\11209\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:55\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     53\u001b[0m         storage \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39mstorage()\u001b[39m.\u001b[39m_new_shared(numel)\n\u001b[0;32m     54\u001b[0m         out \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39mnew(storage)\n\u001b[1;32m---> 55\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mstack(batch, \u001b[39m0\u001b[39;49m, out\u001b[39m=\u001b[39;49mout)\n\u001b[0;32m     56\u001b[0m \u001b[39melif\u001b[39;00m elem_type\u001b[39m.\u001b[39m\u001b[39m__module__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mnumpy\u001b[39m\u001b[39m'\u001b[39m \u001b[39mand\u001b[39;00m elem_type\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mstr_\u001b[39m\u001b[39m'\u001b[39m \\\n\u001b[0;32m     57\u001b[0m         \u001b[39mand\u001b[39;00m elem_type\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mstring_\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m     58\u001b[0m     \u001b[39mif\u001b[39;00m elem_type\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mndarray\u001b[39m\u001b[39m'\u001b[39m \u001b[39mor\u001b[39;00m elem_type\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmemmap\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m     59\u001b[0m         \u001b[39m# array of string classes and object\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(input_tags_size)\n",
    "print(len(X_train_loader)*batch_size)\n",
    "train(fnn_model, optimizer, train_loader=X_train_loader, epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15.624272, 42.92843691148776)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test = TensorDataset(X_test_tensor.to(device), y_test_tensor.to(device))\n",
    "X_test_loader = DataLoader(dataset=dataset_test, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "evaluate(fnn_model, loss_function, eval_loader=X_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 757,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, eval_loader):\n",
    "\n",
    "    #set model in eval mode\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    preds=[]\n",
    "    \n",
    "    # For each batch in our validation set...\n",
    "    for step, (data,) in enumerate(eval_loader):\n",
    "        data = data.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(data)\n",
    "            preds.append(output)\n",
    "\n",
    "    return np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([117990, 267])\n"
     ]
    }
   ],
   "source": [
    "dataset_eval = TensorDataset(X_eval_tensor[:10000].to(device))\n",
    "print(X_eval_tensor.shape)\n",
    "X_eval_loader = DataLoader(dataset=dataset_eval)\n",
    "\n",
    "preds = predict(fnn_model, X_eval_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0533\n"
     ]
    }
   ],
   "source": [
    "preds = np.array([int(np.round(num)) for num in preds ])\n",
    "# write_result(\"data/FNN_predictions.txt\", preds)\n",
    "print(np.mean(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 1164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[\"retweets_count\"].median()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "107bf26d3be942b0376d323aa83938b654b529496a5c3f1b6b3e9c409d104805"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
