{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from verstack.stratified_continuous_split import scsplit # pip install verstack\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from scipy import spatial\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import os\n",
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基本思路\n",
    "使用如下两组要素\n",
    "- 数字变量\n",
    "  1. favorites count (log_stand)\n",
    "  2. followers count (log_stand)\n",
    "  3. statues count (发推数量) (log_stand)\n",
    "  4. friends count (log_stand)\n",
    "  5. urls count (log_stand)\n",
    "  6. verified (1 or 0) (这一部分我们不做log_stand)\n",
    "  7. timestamp (一天内的早中晚（hour），距离大选的时间(month和day)，周几(wday)都可能产生影响)\n",
    "- 文本变量\n",
    "  1. tweet text\n",
    "  2. hashtags \n",
    "\n",
    "将文本变量通过embedding处理后用CNN/LSTM得到一部分输出，另外对数字变量通过CNN也得到了一部分结果，最后汇总，用2层dense layer。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all mentions in train data:  {']', '['}\n",
      "average number of urls in train data: 0.529\n",
      "number of distinct urls in train data:  185951\n",
      "verified users:  10621\n",
      "average number of hashtags in train data: 0.297\n",
      "number of distinct hashtags in train data: 12093\n",
      "average number of hashtags in evaluation data: 0.299\n",
      "number of distinct hashtags in evaluation data: 5942\n",
      "number of tweets not posted in 2022 in train data: 2380\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>retweets_count</th>\n",
       "      <th>favorites_count</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>statuses_count</th>\n",
       "      <th>friends_count</th>\n",
       "      <th>verified</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>urls_count</th>\n",
       "      <th>month</th>\n",
       "      <th>yday</th>\n",
       "      <th>wday</th>\n",
       "      <th>hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[rt, refarcir, macron, ans, nom, prépare]</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3682</td>\n",
       "      <td>453535</td>\n",
       "      <td>3628</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>70</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[populaire]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>86</td>\n",
       "      <td>1016</td>\n",
       "      <td>284</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>78</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[faut, dégager, cinglé]</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1944</td>\n",
       "      <td>28234</td>\n",
       "      <td>1995</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>74</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[enseignants, mettre, prescriptions, président...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1072</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[mafieuse, oppressive, macron]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13957</td>\n",
       "      <td>25311</td>\n",
       "      <td>10841</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  retweets_count  \\\n",
       "0          [rt, refarcir, macron, ans, nom, prépare]               3   \n",
       "1                                        [populaire]               0   \n",
       "2                            [faut, dégager, cinglé]               3   \n",
       "3  [enseignants, mettre, prescriptions, président...               0   \n",
       "4                     [mafieuse, oppressive, macron]               0   \n",
       "\n",
       "   favorites_count  followers_count  statuses_count  friends_count  verified  \\\n",
       "0                0             3682          453535           3628         0   \n",
       "1                0               86            1016            284         0   \n",
       "2                1             1944           28234           1995         0   \n",
       "3                0                1            1072              0         0   \n",
       "4                0            13957           25311          10841         0   \n",
       "\n",
       "  hashtags  urls_count  month  yday  wday  hour  \n",
       "0       []           0      3    70     4     5  \n",
       "1       []           0      3    78     5    12  \n",
       "2       []           0      3    74     1    18  \n",
       "3       []           1      3    73     0    11  \n",
       "4       []           0      3    73     0    11  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "\n",
    "train_data = pd.read_csv(\"data/train.csv\")\n",
    "eval_data = pd.read_csv(\"data/evaluation.csv\")\n",
    "\n",
    "# Pre-process the traning data\n",
    "label = \"text\"\n",
    "train_data[label] = train_data[label].map(lambda x: str.split(x, sep=\" \"))\n",
    "\n",
    "\n",
    "label = \"mentions\"\n",
    "train_data[label+\"_count\"] = train_data[label].map(lambda x: len(str.split(x)))\n",
    "mentions = set()\n",
    "train_data[label].apply(lambda x: mentions.update(x))\n",
    "print(\"all mentions in train data: \", mentions)\n",
    "\n",
    "\n",
    "label = \"urls\"\n",
    "train_data[label] = train_data[label].map(lambda x: [] if x==\"[]\" else [str.strip(url) for url in str.split(x[1:-1], sep=\",\")])\n",
    "train_data[label+\"_count\"] = train_data[label].map(lambda x: len(x))\n",
    "urls = set()\n",
    "train_data[label].apply(lambda x: None if len(x)<1 else urls.update(x))\n",
    "train_data[label].apply(lambda x: None if len(x)<1 else hashtags.update(x))\n",
    "print(f\"average number of urls in train data: {train_data.urls_count.mean():1.3f}\")\n",
    "print(\"number of distinct urls in train data: \", len(urls))\n",
    "\n",
    "label = \"verified\"\n",
    "print(\"verified users: \", train_data[\"verified\"].sum())\n",
    "\n",
    "\n",
    "label = \"hashtags\"\n",
    "train_data[label] = train_data[label].map(lambda x: [] if x==\"[]\" else [str.strip(tag) for tag in str.split(x[1:-1], sep=\",\")])\n",
    "train_data[label+\"_count\"] = train_data[label].map(lambda x: len(x))\n",
    "hashtags = set()\n",
    "train_data[label].apply(lambda x: None if len(x)<1 else hashtags.update(x))\n",
    "print(f\"average number of hashtags in train data: {train_data.hashtags_count.mean():1.3f}\")\n",
    "print(\"number of distinct hashtags in train data:\", len(hashtags))\n",
    "\n",
    "\n",
    "label = \"hashtags\"\n",
    "eval_data[label] = eval_data[label].map(lambda x: [] if x==\"[]\" else [str.strip(tag) for tag in str.split(x[1:-1], sep=\",\")])\n",
    "eval_data[label+\"_count\"] = eval_data[label].map(lambda x: len(x))\n",
    "hashtags2 = set()\n",
    "eval_data[label].apply(lambda x: None if not x else hashtags2.update(x))\n",
    "print(f\"average number of hashtags in evaluation data: {eval_data.hashtags_count.mean():1.3f}\")\n",
    "print(\"number of distinct hashtags in evaluation data:\", len(hashtags2))\n",
    "\n",
    "\n",
    "\n",
    "# Treatment of time\n",
    "# The time relative to the election (month and yday), day in the week, \n",
    "# and the hour all affect the number of retweets.\n",
    "train_data[\"timestamp\"] = train_data[\"timestamp\"].map(lambda x: time.gmtime(x//1000))\n",
    "# Only 2380 tweets are not posted in 2022, so we ignore the year\n",
    "# train_data[\"year\"] = train_data[\"timestamp\"].map(lambda x: x.tm_year)\n",
    "train_data[\"month\"] = train_data[\"timestamp\"].map(lambda x: x.tm_mon)\n",
    "train_data[\"yday\"] = train_data[\"timestamp\"].map(lambda x: x.tm_yday)\n",
    "train_data[\"wday\"] = train_data[\"timestamp\"].map(lambda x: x.tm_wday)\n",
    "train_data[\"hour\"] = train_data[\"timestamp\"].map(lambda x: x.tm_hour)\n",
    "print(\"number of tweets not posted in 2022 in train data:\", len(train_data[train_data[\"timestamp\"].map(lambda x: x.tm_year) < 2022]))\n",
    "\n",
    "# We drop the following data:\n",
    "# @TweetID: useless\n",
    "# @mentions: none of the mentions in the train data are not null\n",
    "# @mentions_count\n",
    "# @hashtags_count: we use hashtags directly\n",
    "# @urls: we use urls_count instead\n",
    "\n",
    "train_data.drop(labels=[\"TweetID\",\"timestamp\", \"mentions\",\"mentions_count\", \"hashtags_count\", \"urls\"], axis=1, inplace=True)\n",
    "train_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we split our training data into trainig and testing set. This way we can estimate the evaluation of our model without uploading to Kaggle and avoid overfitting over our evaluation dataset.\n",
    "# scsplit method is used in order to split our regression data in a stratisfied way and keep a similar distribution of retweet counts between the two sets\n",
    "X_train, X_test, y_train, y_test = scsplit(train_data, train_data['retweets_count'], stratify=train_data['retweets_count'], train_size=0.7, test_size=0.3)\n",
    "\n",
    "# We remove the actual number of retweets from our features since it is the value that we are trying to predict\n",
    "X_train = X_train.drop([\"retweets_count\"], axis=1)\n",
    "X_test = X_test.drop([\"retweets_count\"], axis=1)\n",
    "\n",
    "# log transform\n",
    "def log_transform(data, list_column):\n",
    "        for name_column in list_column:\n",
    "            data[name_column] = np.log(data[name_column])\n",
    "\n",
    "# standardise the columns of numeric values\n",
    "def standardise(data,list_column):\n",
    "    for name_column in list_column:\n",
    "        # standardize\n",
    "        if data[name_column].std()!=0:\n",
    "            data[name_column]=(data[name_column]-data[name_column].mean())/data[name_column].std()\n",
    "        else:\n",
    "            data[name_column]=(data[name_column]-data[name_column].mean())\n",
    "\n",
    "standardise(X_train,[\"favorites_count\",\"followers_count\",\"statuses_count\",\"friends_count\",\"urls_count\",\"month\",\"yday\",\"wday\",\"hour\"])\n",
    "standardise(X_test, [\"favorites_count\",\"followers_count\",\"statuses_count\",\"friends_count\",\"urls_count\",\"month\",\"yday\",\"wday\",\"hour\"])\n",
    "\n",
    "standardise(X_train,[\"favorites_count\",\"followers_count\",\"statuses_count\",\"friends_count\",\"urls_count\",\"month\",\"yday\",\"wday\",\"hour\"])\n",
    "standardise(X_test, [\"favorites_count\",\"followers_count\",\"statuses_count\",\"friends_count\",\"urls_count\",\"month\",\"yday\",\"wday\",\"hour\"])\n",
    "\n",
    "# split the table into 2 parts: one with the text and the other with the numbers\n",
    "X_train_num=X_train.drop([\"text\", \"hashtags\"], axis=1)\n",
    "X_test_num=X_test.drop([\"text\", \"hashtags\"], axis=1)\n",
    "\n",
    "X_train_text=X_train[[\"text\", \"hashtags\"]]\n",
    "X_test_text=X_test[[\"text\", \"hashtags\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part is used to train the embedding of the vocabulary, only needed to be run once\n",
    "\n",
    "path_text_dataset='French-Word-Embeddings/Data/data.txt'\n",
    "text_dataset=pd.read_table(path_text_dataset).values\n",
    "text_dataset=[item[0][:-1] for item in text_dataset]\n",
    "\n",
    "TaggedDocument=gensim.models.doc2vec.TaggedDocument\n",
    "\n",
    "def X_text(sentences):\n",
    "    X=[]\n",
    "    for i,text in enumerate(sentences):\n",
    "        words=text.split(\" \")\n",
    "        l=len(words)\n",
    "        words=words[:-2]\n",
    "        document=TaggedDocument(words,tags=[i])\n",
    "        X.append(document)\n",
    "    return X\n",
    "\n",
    "X_documents=X_text(text_dataset)\n",
    "X_documents[:5]\n",
    "\n",
    "def train_text(text_train,size=10,epochs=10):\n",
    "    model=Doc2Vec(text_train,min_count=1,window=3,vector_size=size,sample=1e-3,negative=5,epochs=epochs)\n",
    "    model.train(text_train,total_examples=model.corpus_count,epochs=model.epochs)\n",
    "    return model\n",
    "\n",
    "if not os.path.exists('./WE_models'):\n",
    "    os.mkdir('./WE_models')\n",
    "\n",
    "# run only once this part  \n",
    "# model_text10=train_text(X_documents, size=10)\n",
    "# model_text10.save('WE_models/d2v_10D')\n",
    "# model_text5=train_text(X_documents, size=5)\n",
    "# model_text5.save('WE_models/d2v_5D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we load the embedding model:\n",
    "model_text10 = Doc2Vec.load('WE_models/d2v_10D')\n",
    "model_text5 = Doc2Vec.load('WE_models/d2v_5D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([247778, 15]), torch.Size([106191, 15]))"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tranform the textual part to vector\n",
    "\n",
    "def text2vec(text_train, model, alpha=0.05, min_alpha=0.025, epochs=10, tags=True):\n",
    "    list_text=[]\n",
    "    for sentence in text_train:\n",
    "        vec = model.infer_vector(doc_words=sentence, alpha=alpha, min_alpha=min_alpha, epochs=epochs).tolist()\n",
    "        list_text.append(vec)\n",
    "    return np.array(list_text)\n",
    "\n",
    "# text to vector\n",
    "train_text_tensor=torch.Tensor(text2vec(X_train_text[\"text\"], model=model_text10))\n",
    "test_text_tensor=torch.Tensor(text2vec(X_test_text[\"text\"], model=model_text10))\n",
    "\n",
    "# hashtags to vector\n",
    "train_tags_tensor=torch.Tensor(text2vec(X_train_text[\"hashtags\"], model=model_text5))\n",
    "test_tags_tensor=torch.Tensor(text2vec(X_test_text[\"hashtags\"], model=model_text5))\n",
    "\n",
    "# combine the two vectors\n",
    "X_train_text_tensor = torch.cat((train_text_tensor, train_tags_tensor), axis=1)\n",
    "X_test_text_tensor = torch.cat((test_text_tensor, test_tags_tensor), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([247778, 10]) torch.Size([247778, 1])\n",
      "torch.Size([247778, 15]) torch.Size([106191, 15])\n",
      "torch.Size([247778, 25]) torch.Size([106191, 25])\n"
     ]
    }
   ],
   "source": [
    "X_train_num_tensor=torch.Tensor(X_train_num.values)\n",
    "X_test_num_tensor=torch.Tensor(X_test_num.values)\n",
    "\n",
    "X_train_tensor=torch.cat((X_train_num_tensor, X_train_text_tensor), axis=1)\n",
    "X_test_tensor=torch.cat((X_test_num_tensor, X_test_text_tensor), axis=1)\n",
    "\n",
    "y_train_tensor=torch.Tensor(y_train.to_numpy()).reshape((-1,1))\n",
    "y_test_tensor=torch.Tensor(y_test.to_numpy()).reshape((-1,1))\n",
    "\n",
    "print(X_train_num_tensor.shape, y_train_tensor.shape)\n",
    "print(X_train_text_tensor.shape, X_test_text_tensor.shape)\n",
    "print(X_train_tensor.shape, X_test_tensor.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "107bf26d3be942b0376d323aa83938b654b529496a5c3f1b6b3e9c409d104805"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
